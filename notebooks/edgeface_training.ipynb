{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34595,"sourceType":"datasetVersion","datasetId":26922}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install timm==0.9.12\n!git clone https://gitlab.idiap.ch/bob/bob.paper.tbiom2023_edgeface.git\n%cd bob.paper.tbiom2023_edgeface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:51:47.482437Z","iopub.execute_input":"2025-05-25T02:51:47.482680Z","iopub.status.idle":"2025-05-25T02:53:37.327616Z","shell.execute_reply.started":"2025-05-25T02:51:47.482658Z","shell.execute_reply":"2025-05-25T02:53:37.326834Z"}},"outputs":[{"name":"stdout","text":"Collecting timm==0.9.12\n  Downloading timm-0.9.12-py3-none-any.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.11/dist-packages (from timm==0.9.12) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.9.12) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm==0.9.12) (6.0.2)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from timm==0.9.12) (0.31.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm==0.9.12) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7->timm==0.9.12)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm==0.9.12) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7->timm==0.9.12) (1.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.12) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.12) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.12) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.12) (1.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.9.12) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.9.12) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7->timm==0.9.12) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.9.12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.9.12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.9.12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.9.12) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.9.12) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.9.12) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm==0.9.12) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm==0.9.12) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm==0.9.12) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm==0.9.12) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm==0.9.12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm==0.9.12) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm==0.9.12) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm==0.9.12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm==0.9.12) (2024.2.0)\nDownloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.15\n    Uninstalling timm-1.0.15:\n      Successfully uninstalled timm-1.0.15\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 timm-0.9.12\nCloning into 'bob.paper.tbiom2023_edgeface'...\nremote: Enumerating objects: 200, done.\u001b[K\nremote: Counting objects: 100% (49/49), done.\u001b[K\nremote: Compressing objects: 100% (48/48), done.\u001b[K\nremote: Total 200 (delta 21), reused 0 (delta 0), pack-reused 151 (from 1)\u001b[K\nReceiving objects: 100% (200/200), 126.02 MiB | 4.23 MiB/s, done.\nResolving deltas: 100% (64/64), done.\n/kaggle/working/bob.paper.tbiom2023_edgeface\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom backbones import get_model , replace_linear_with_lowrank_2, get_timmfrv2\nimport numpy as np\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:53:37.331506Z","iopub.execute_input":"2025-05-25T02:53:37.332048Z","iopub.status.idle":"2025-05-25T02:53:46.376131Z","shell.execute_reply.started":"2025-05-25T02:53:37.332004Z","shell.execute_reply":"2025-05-25T02:53:46.375562Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# LFW Dataset Filtration ","metadata":{}},{"cell_type":"code","source":"import os\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom collections import defaultdict\nimport shutil\n\ndef filter_classes_by_min_images(root_dir, write_dir, min_images=5):\n    filtered_dir = os.path.join(write_dir, f'filtered_min{min_images}')\n    os.makedirs(filtered_dir, exist_ok=True)\n\n    for class_name in os.listdir(root_dir):\n        class_path = os.path.join(root_dir, class_name)\n        if not os.path.isdir(class_path):\n            continue\n        images = os.listdir(class_path)\n        if len(images) >= min_images:\n            os.makedirs(os.path.join(filtered_dir, class_name), exist_ok=True)\n            for img in images:\n                src = os.path.join(class_path, img)\n                dst = os.path.join(filtered_dir, class_name, img)\n                if not os.path.exists(dst):  # Avoid overwrite\n                    shutil.copy2(src, dst)\n\n    return filtered_dir\n\n# Example usage:\nroot_dir = \"/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled/\"\nwrite_dir= \"/kaggle/working/\"\nmin_required_images = 10\nfiltered_root = filter_classes_by_min_images(root_dir,write_dir, min_required_images)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:54:02.165190Z","iopub.execute_input":"2025-05-25T02:54:02.165981Z","iopub.status.idle":"2025-05-25T02:55:41.132031Z","shell.execute_reply.started":"2025-05-25T02:54:02.165946Z","shell.execute_reply":"2025-05-25T02:55:41.131190Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"filtered_root = '/kaggle/working/filtered_min10'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:11:37.600673Z","iopub.execute_input":"2025-05-24T10:11:37.601398Z","iopub.status.idle":"2025-05-24T10:11:37.604681Z","shell.execute_reply.started":"2025-05-24T10:11:37.601368Z","shell.execute_reply":"2025-05-24T10:11:37.603813Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!ls /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:55:41.133321Z","iopub.execute_input":"2025-05-25T02:55:41.133748Z","iopub.status.idle":"2025-05-25T02:55:41.268377Z","shell.execute_reply.started":"2025-05-25T02:55:41.133727Z","shell.execute_reply":"2025-05-25T02:55:41.267713Z"}},"outputs":[{"name":"stdout","text":"bob.paper.tbiom2023_edgeface  filtered_min10\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# DataLoaders Setup","metadata":{}},{"cell_type":"code","source":"def getDataset(root_dir):\n    transform = transforms.Compose([\n            transforms.Resize((112, 112)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        \n    dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:56:36.265636Z","iopub.execute_input":"2025-05-25T02:56:36.266041Z","iopub.status.idle":"2025-05-25T02:56:36.271899Z","shell.execute_reply.started":"2025-05-25T02:56:36.265990Z","shell.execute_reply":"2025-05-25T02:56:36.271140Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Class-Aware Dataset Partitioning","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom torch.utils.data import Subset, DataLoader\nimport random\n\ndef stratified_split(dataset, val_ratio=0.1, test_ratio=0.1, seed=42):\n    label_to_indices = defaultdict(list)\n    for idx, (_, label) in enumerate(dataset):\n        label_to_indices[label].append(idx)\n\n    train_indices, val_indices, test_indices = [], [], []\n    rng = random.Random(seed)\n\n    for label, indices in label_to_indices.items():\n        rng.shuffle(indices)\n        total = len(indices)\n\n        val_count = max(2, int(total * val_ratio))\n        test_count = max(2, int(total * test_ratio))\n        train_count = total - val_count - test_count\n\n        if train_count < 1:\n            train_count, val_count, test_count = max(1, total - 2), 1, 1\n\n        train_indices += indices[:train_count]\n        val_indices += indices[train_count:train_count + val_count]\n        test_indices += indices[train_count + val_count:]\n\n    return Subset(dataset, train_indices), Subset(dataset, val_indices), Subset(dataset, test_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:56:40.591039Z","iopub.execute_input":"2025-05-25T02:56:40.591601Z","iopub.status.idle":"2025-05-25T02:56:40.598044Z","shell.execute_reply.started":"2025-05-25T02:56:40.591575Z","shell.execute_reply":"2025-05-25T02:56:40.597351Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Triplet Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset\n\nclass TripletDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.label_to_indices = self._build_label_index()\n\n    def _build_label_index(self):\n        label_to_indices = defaultdict(list)\n        for idx, (_, label) in enumerate(self.dataset):\n            label_to_indices[label].append(idx)\n        return label_to_indices\n\n    def __getitem__(self, index):\n        anchor_img, anchor_label = self.dataset[index]\n\n        # Positive\n        pos_idx = index\n        while pos_idx == index:\n            pos_idx = random.choice(self.label_to_indices[anchor_label])\n        positive_img, _ = self.dataset[pos_idx]\n\n        # Negative\n        neg_label = anchor_label\n        while neg_label == anchor_label:\n            neg_label = random.choice(list(self.label_to_indices.keys()))\n        neg_idx = random.choice(self.label_to_indices[neg_label])\n        negative_img, _ = self.dataset[neg_idx]\n\n        return anchor_img, positive_img, negative_img\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:56:45.177386Z","iopub.execute_input":"2025-05-25T02:56:45.178192Z","iopub.status.idle":"2025-05-25T02:56:45.184408Z","shell.execute_reply.started":"2025-05-25T02:56:45.178166Z","shell.execute_reply":"2025-05-25T02:56:45.183592Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Pairs Dataset","metadata":{}},{"cell_type":"code","source":"class PairDataset(Dataset):\n    def __init__(self, dataset, num_pairs=1000):\n        self.dataset = dataset\n        self.label_to_indices = self._build_label_index()\n        self.num_pairs = num_pairs\n        self.pairs = self._generate_pairs()\n\n    def _build_label_index(self):\n        label_to_indices = defaultdict(list)\n        for idx, (_, label) in enumerate(self.dataset):\n            label_to_indices[label].append(idx)\n        return label_to_indices\n        \n    def _generate_pairs(self):\n        pairs = []\n        for _ in range(self.num_pairs // 2):\n            label = random.choice(list(self.label_to_indices.keys()))\n            i1, i2 = random.sample(self.label_to_indices[label], 2)\n            pairs.append((i1, i2, 1))  # positive pair\n\n            label1, label2 = random.sample(list(self.label_to_indices.keys()), 2)\n            i1 = random.choice(self.label_to_indices[label1])\n            i2 = random.choice(self.label_to_indices[label2])\n            pairs.append((i1, i2, 0))  # negative pair\n\n        return pairs\n\n    def __getitem__(self, index):\n        i1, i2, label = self.pairs[index]\n        img1, _ = self.dataset[i1]\n        img2, _ = self.dataset[i2]\n        return img1, img2, torch.tensor(label, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:56:49.502707Z","iopub.execute_input":"2025-05-25T02:56:49.503017Z","iopub.status.idle":"2025-05-25T02:56:49.511053Z","shell.execute_reply.started":"2025-05-25T02:56:49.502992Z","shell.execute_reply":"2025-05-25T02:56:49.510093Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def get_triplet_dataloaders(\n    dataset,\n    batch_size,\n    num_workers=2,\n    val_ratio=0.1,\n    test_ratio=0.1,\n    seed=42\n):\n    train_set, val_set, test_set = stratified_split(dataset, val_ratio, test_ratio, seed)\n\n    triplet_train = TripletDataset(train_set)\n    pair_val = PairDataset(val_set, num_pairs=1000)\n    pair_test = PairDataset(test_set, num_pairs=1000)\n\n    train_loader = DataLoader(triplet_train, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True, drop_last=True)\n\n    val_loader = DataLoader(pair_val, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    test_loader = DataLoader(pair_test, batch_size=batch_size, shuffle=False,\n                             num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:57:03.798979Z","iopub.execute_input":"2025-05-25T02:57:03.799228Z","iopub.status.idle":"2025-05-25T02:57:03.804576Z","shell.execute_reply.started":"2025-05-25T02:57:03.799212Z","shell.execute_reply":"2025-05-25T02:57:03.803784Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_classif_dataloaders(\n    dataset,\n    batch_size,\n    num_workers=2,\n    val_ratio=0.1,\n    test_ratio=0.1,\n    seed=42\n):\n    train_set, val_set, test_set = stratified_split(dataset, val_ratio, test_ratio, seed)\n\n    triplet_train = train_set\n    pair_val = PairDataset(val_set, num_pairs=1000)\n    pair_test = PairDataset(test_set, num_pairs=1000)\n\n    train_loader = DataLoader(triplet_train, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True, drop_last=True)\n\n    val_loader = DataLoader(pair_val, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    test_loader = DataLoader(pair_test, batch_size=batch_size, shuffle=False,\n                             num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:57:07.237064Z","iopub.execute_input":"2025-05-25T02:57:07.237366Z","iopub.status.idle":"2025-05-25T02:57:07.242711Z","shell.execute_reply.started":"2025-05-25T02:57:07.237345Z","shell.execute_reply":"2025-05-25T02:57:07.242080Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = getDataset(root_dir=filtered_root)\ntriplet_train_loader, triplet_val_loader, triplet_test_loader = get_triplet_dataloaders(\n    dataset, batch_size=128)\n\ntrain_loader, val_loader, test_loader = get_classif_dataloaders(\n    dataset, batch_size=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:57:20.162834Z","iopub.execute_input":"2025-05-25T02:57:20.163476Z","iopub.status.idle":"2025-05-25T02:57:39.722057Z","shell.execute_reply.started":"2025-05-25T02:57:20.163449Z","shell.execute_reply":"2025-05-25T02:57:39.721262Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"len(triplet_train_loader), len(triplet_val_loader), len(triplet_test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:57:39.723234Z","iopub.execute_input":"2025-05-25T02:57:39.723506Z","iopub.status.idle":"2025-05-25T02:57:39.729649Z","shell.execute_reply.started":"2025-05-25T02:57:39.723488Z","shell.execute_reply":"2025-05-25T02:57:39.729005Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(26, 8, 8)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Model Architecture Overview","metadata":{}},{"cell_type":"code","source":"model = replace_linear_with_lowrank_2(\n            get_timmfrv2('edgenext_small',featdim=512), rank_ratio=0.5)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T02:57:46.761808Z","iopub.execute_input":"2025-05-25T02:57:46.762331Z","iopub.status.idle":"2025-05-25T02:57:46.923950Z","shell.execute_reply.started":"2025-05-25T02:57:46.762306Z","shell.execute_reply":"2025-05-25T02:57:46.923309Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TimmFRWrapperV2(\n  (model): EdgeNeXt(\n    (stem): Sequential(\n      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((48,), eps=1e-06, elementwise_affine=True)\n    )\n    (stages): Sequential(\n      (0): EdgeNeXtStage(\n        (downsample): Identity()\n        (blocks): Sequential(\n          (0): ConvBlock(\n            (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n            (norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=48, out_features=24, bias=False)\n                (linear2): Linear(in_features=24, out_features=192, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=192, out_features=24, bias=False)\n                (linear2): Linear(in_features=24, out_features=48, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (1): ConvBlock(\n            (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n            (norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=48, out_features=24, bias=False)\n                (linear2): Linear(in_features=24, out_features=192, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=192, out_features=24, bias=False)\n                (linear2): Linear(in_features=24, out_features=48, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (2): ConvBlock(\n            (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n            (norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=48, out_features=24, bias=False)\n                (linear2): Linear(in_features=24, out_features=192, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=192, out_features=24, bias=False)\n                (linear2): Linear(in_features=24, out_features=48, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n        )\n      )\n      (1): EdgeNeXtStage(\n        (downsample): Sequential(\n          (0): LayerNorm2d((48,), eps=1e-06, elementwise_affine=True)\n          (1): Conv2d(48, 96, kernel_size=(2, 2), stride=(2, 2))\n        )\n        (blocks): Sequential(\n          (0): ConvBlock(\n            (conv_dw): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=96, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=384, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=384, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=96, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (1): ConvBlock(\n            (conv_dw): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)\n            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=96, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=384, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=384, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=96, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (2): SplitTransposeBlock(\n            (convs): ModuleList(\n              (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n            )\n            (pos_embd): PositionalEncodingFourier(\n              (token_projection): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n            )\n            (norm_xca): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n            (xca): CrossCovarianceAttn(\n              (qkv): LoRaLin(\n                (linear1): Linear(in_features=96, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=288, bias=True)\n              )\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): LoRaLin(\n                (linear1): Linear(in_features=96, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=96, bias=True)\n              )\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=96, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=384, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=384, out_features=48, bias=False)\n                (linear2): Linear(in_features=48, out_features=96, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n        )\n      )\n      (2): EdgeNeXtStage(\n        (downsample): Sequential(\n          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n          (1): Conv2d(96, 160, kernel_size=(2, 2), stride=(2, 2))\n        )\n        (blocks): Sequential(\n          (0): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (1): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (2): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (3): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (4): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (5): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (6): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (7): ConvBlock(\n            (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (8): SplitTransposeBlock(\n            (convs): ModuleList(\n              (0-1): 2 x Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=54)\n            )\n            (norm_xca): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (xca): CrossCovarianceAttn(\n              (qkv): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=480, bias=True)\n              )\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=160, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=640, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=640, out_features=80, bias=False)\n                (linear2): Linear(in_features=80, out_features=160, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n        )\n      )\n      (3): EdgeNeXtStage(\n        (downsample): Sequential(\n          (0): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n          (1): Conv2d(160, 304, kernel_size=(2, 2), stride=(2, 2))\n        )\n        (blocks): Sequential(\n          (0): ConvBlock(\n            (conv_dw): Conv2d(304, 304, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=304)\n            (norm): LayerNorm((304,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=304, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=1216, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=1216, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=304, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (1): ConvBlock(\n            (conv_dw): Conv2d(304, 304, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=304)\n            (norm): LayerNorm((304,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=304, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=1216, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=1216, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=304, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n          (2): SplitTransposeBlock(\n            (convs): ModuleList(\n              (0-2): 3 x Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=76)\n            )\n            (norm_xca): LayerNorm((304,), eps=1e-06, elementwise_affine=True)\n            (xca): CrossCovarianceAttn(\n              (qkv): LoRaLin(\n                (linear1): Linear(in_features=304, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=912, bias=True)\n              )\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): LoRaLin(\n                (linear1): Linear(in_features=304, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=304, bias=True)\n              )\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (norm): LayerNorm((304,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): LoRaLin(\n                (linear1): Linear(in_features=304, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=1216, bias=True)\n              )\n              (act): GELU(approximate='none')\n              (drop1): Dropout(p=0.0, inplace=False)\n              (norm): Identity()\n              (fc2): LoRaLin(\n                (linear1): Linear(in_features=1216, out_features=152, bias=False)\n                (linear2): Linear(in_features=152, out_features=304, bias=True)\n              )\n              (drop2): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n          )\n        )\n      )\n    )\n    (norm_pre): Identity()\n    (head): Sequential(\n      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n      (norm): LayerNorm2d((304,), eps=1e-06, elementwise_affine=True)\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (drop): Dropout(p=0.0, inplace=False)\n      (fc): LoRaLin(\n        (linear1): Linear(in_features=304, out_features=152, bias=False)\n        (linear2): Linear(in_features=152, out_features=512, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Loss Function Comparison","metadata":{}},{"cell_type":"code","source":"model_name='edgeface_s_gamma_05'\nembedding_size=512\nmargin_list = (1.0, 0.0, 0.4)\nnum_classes = len(dataset.classes)\nsample_rate = 1\nlr = 1e-3\nweight_decay = 0.05","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:49:01.852523Z","iopub.execute_input":"2025-05-25T03:49:01.853230Z","iopub.status.idle":"2025-05-25T03:49:01.857096Z","shell.execute_reply.started":"2025-05-25T03:49:01.853206Z","shell.execute_reply":"2025-05-25T03:49:01.856456Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import torch\nfrom torch.nn.functional import normalize, linear\nfrom typing import Callable\nimport math\n\n\nclass CombinedMarginLoss(torch.nn.Module):\n    def __init__(self, s: float, m1: float, m2: float, m3: float):\n        super().__init__()\n        self.s = s\n        self.m1 = m1\n        self.m2 = m2\n        self.m3 = m3\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        one_hot = torch.zeros_like(logits)\n        one_hot.scatter_(1, labels.view(-1, 1), 1)\n\n        # cosine similarity values (logits) must be clamped for arccos stability\n        cosine = logits.clamp(-1 + 1e-7, 1 - 1e-7)\n        theta = cosine.acos()\n\n        # Apply angular margin (ArcFace or SphereFace)\n        if self.m1 != 1.0 or self.m2 != 0.0:\n            theta = self.m1 * theta + self.m2\n            target_logits = theta.cos()\n        else:\n            target_logits = cosine\n\n        # Apply additive cosine margin (CosFace)\n        if self.m3 > 0.0:\n            target_logits -= self.m3\n\n        # Update logits for the ground-truth classes\n        logits = logits.clone()\n        logits[one_hot.bool()] = target_logits[one_hot.bool()]\n\n        # Apply scale\n        logits *= self.s\n        return logits\n\n\nclass SimplePartialFC(torch.nn.Module):\n    def __init__(\n        self,\n        margin_loss: Callable,\n        embedding_size: int,\n        num_classes: int,\n        sample_rate: float = 1.0,\n        fp16: bool = False,\n    ):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.num_classes = num_classes\n        self.sample_rate = sample_rate\n        self.fp16 = fp16\n\n        self.weight = torch.nn.Parameter(torch.randn(num_classes, embedding_size) * 0.01)\n        self.margin_softmax = margin_loss\n        self.ce_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor):\n        labels = labels.long().view(-1)\n\n        if self.sample_rate < 1.0:\n            with torch.no_grad():\n                positive = torch.unique(labels)\n                num_sample = int(self.sample_rate * self.num_classes)\n                all_indices = torch.randperm(self.num_classes, device=embeddings.device)\n                neg_sample = all_indices[~torch.isin(all_indices, positive)][: max(0, num_sample - len(positive))]\n                sample_indices = torch.cat([positive, neg_sample])\n                sample_indices, _ = sample_indices.sort()\n                weight = self.weight[sample_indices]\n                label_map = {old.item(): new for new, old in enumerate(sample_indices)}\n                labels = torch.tensor([label_map[l.item()] for l in labels], device=labels.device)\n            logits = linear(normalize(embeddings), normalize(weight))\n        else:\n            logits = linear(normalize(embeddings), normalize(self.weight))\n\n        logits = logits.clamp(-1, 1)\n        logits = self.margin_softmax(logits, labels)\n        return self.ce_loss(logits, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:49:13.219780Z","iopub.execute_input":"2025-05-25T03:49:13.220066Z","iopub.status.idle":"2025-05-25T03:49:13.230927Z","shell.execute_reply.started":"2025-05-25T03:49:13.220046Z","shell.execute_reply":"2025-05-25T03:49:13.230198Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class LFWTrainer:\n    def __init__(self, model_name, \n                 embedding_size, train_loader, \n                 val_loader, margin_list=margin_list):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Model\n        self.model = get_model(model_name, dropout=0.0, num_features=embedding_size)\n        self.model = self.model.to(self.device)\n\n        self.num_classes = num_classes\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n  \n        # Loss\n        # self.criterion = ArcFace(64, 0.5).to(self.device)\n        self.criterion = CombinedMarginLoss(64, margin_list[0], margin_list[1], margin_list[2])\n        \n        self.module_partial_fc = SimplePartialFC(self.criterion, embedding_size, \n                                         num_classes, sample_rate, False)\n        self.module_partial_fc.train().cuda()\n\n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            params=[{\"params\": self.model.parameters()}, \n                    {\"params\": self.module_partial_fc.parameters()}],\n            lr=lr, weight_decay=weight_decay)\n         \n        # self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=12, verbose=True)\n\n        # Transform\n        self.transform = transforms.Compose([\n            transforms.Resize((112, 112)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        \n    def train(self, num_epochs=10):\n        for epoch in range(num_epochs):\n            self.model.train()\n            running_loss = 0.0\n            \n            for i, (images, labels) in enumerate(self.train_loader):\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                \n                embeddings = self.model(images)\n                loss: torch.Tensor = self.module_partial_fc(embeddings, labels)\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                \n                running_loss += loss.item()\n                \n                # if i % 10 == 0:\n                #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(self.train_loader)}], Loss: {loss.item():.4f}')\n            \n            # Validate\n            val_acc = self.validate()\n            current_lr = self.optimizer.param_groups[0]['lr']\n            epoch_loss = running_loss / len(self.train_loader)\n            print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f},'\n            f', LR: {current_lr:.6f}')\n\n            self.scheduler.step(val_acc)\n            \n    def validate(self):\n        self.model.eval()\n        all_sims = []\n        all_labels = []\n    \n        with torch.no_grad():\n            for img1, img2, label in self.val_loader:\n                img1, img2 = img1.to(self.device), img2.to(self.device)\n                label = label.to(self.device)\n    \n                emb1 = F.normalize(self.model(img1))\n                emb2 = F.normalize(self.model(img2))\n                sim = F.cosine_similarity(emb1, emb2)\n    \n                all_sims.extend(sim.cpu().numpy())\n                all_labels.extend(label.cpu().numpy())\n    \n        all_sims = np.array(all_sims)\n        all_labels = np.array(all_labels)\n    \n        # Find the best threshold\n        best_acc = 0.0\n        best_thresh = 0.0\n        for thresh in np.arange(0, 1.01, 0.01):\n            preds = (all_sims > thresh).astype(int)\n            acc = (preds == all_labels).mean()\n            if acc > best_acc:\n                best_acc = acc\n                best_thresh = thresh\n        return best_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:12:23.144667Z","iopub.execute_input":"2025-05-25T05:12:23.144925Z","iopub.status.idle":"2025-05-25T05:12:23.157186Z","shell.execute_reply.started":"2025-05-25T05:12:23.144906Z","shell.execute_reply":"2025-05-25T05:12:23.156578Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"def test(model, loader):\n    model.eval()\n    all_sims = []\n    all_labels = []\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    with torch.no_grad():\n        for img1, img2, label in loader:\n            img1, img2 = img1.to(device), img2.to(device)\n            label = label.to(device)\n\n            emb1 = F.normalize(model(img1))\n            emb2 = F.normalize(model(img2))\n            sim = F.cosine_similarity(emb1, emb2)\n\n            all_sims.extend(sim.cpu().numpy())\n            all_labels.extend(label.cpu().numpy())\n\n    all_sims = np.array(all_sims)\n    all_labels = np.array(all_labels)\n\n    # Find the best threshold\n    best_acc = 0.0\n    best_thresh = 0.0\n    for thresh in np.arange(0, 1.01, 0.01):\n        preds = (all_sims > thresh).astype(int)\n        acc = (preds == all_labels).mean()\n        if acc > best_acc:\n            best_acc = acc\n            best_thresh = thresh\n    return best_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:49:14.614471Z","iopub.execute_input":"2025-05-25T03:49:14.614937Z","iopub.status.idle":"2025-05-25T03:49:14.620897Z","shell.execute_reply.started":"2025-05-25T03:49:14.614913Z","shell.execute_reply":"2025-05-25T03:49:14.620168Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"trainer=LFWTrainer(model_name = model_name, \n                    embedding_size = embedding_size, \n                    train_loader = train_loader, \n                     val_loader = val_loader,\n                  \n                  )\n\ntrainer.train(num_epochs=120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:49:24.731241Z","iopub.execute_input":"2025-05-25T03:49:24.731992Z","iopub.status.idle":"2025-05-25T04:04:39.610661Z","shell.execute_reply.started":"2025-05-25T03:49:24.731968Z","shell.execute_reply":"2025-05-25T04:04:39.609885Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/120] Loss: 33.0116, Val Acc: 0.5150,, LR: 0.001000\nEpoch [2/120] Loss: 30.9069, Val Acc: 0.5010,, LR: 0.001000\nEpoch [3/120] Loss: 30.6641, Val Acc: 0.5040,, LR: 0.001000\nEpoch [4/120] Loss: 30.5675, Val Acc: 0.5120,, LR: 0.001000\nEpoch [5/120] Loss: 30.4162, Val Acc: 0.5440,, LR: 0.001000\nEpoch [6/120] Loss: 30.1792, Val Acc: 0.5880,, LR: 0.001000\nEpoch [7/120] Loss: 29.8838, Val Acc: 0.6170,, LR: 0.001000\nEpoch [8/120] Loss: 29.5270, Val Acc: 0.6420,, LR: 0.001000\nEpoch [9/120] Loss: 29.1677, Val Acc: 0.6210,, LR: 0.001000\nEpoch [10/120] Loss: 28.9309, Val Acc: 0.6200,, LR: 0.001000\nEpoch [11/120] Loss: 28.2442, Val Acc: 0.6480,, LR: 0.001000\nEpoch [12/120] Loss: 27.3225, Val Acc: 0.6010,, LR: 0.001000\nEpoch [13/120] Loss: 26.0842, Val Acc: 0.6190,, LR: 0.001000\nEpoch [14/120] Loss: 24.6807, Val Acc: 0.6240,, LR: 0.001000\nEpoch [15/120] Loss: 23.9559, Val Acc: 0.6240,, LR: 0.001000\nEpoch [16/120] Loss: 23.2427, Val Acc: 0.6260,, LR: 0.001000\nEpoch [17/120] Loss: 22.0934, Val Acc: 0.6420,, LR: 0.001000\nEpoch [18/120] Loss: 21.3945, Val Acc: 0.6470,, LR: 0.001000\nEpoch [19/120] Loss: 20.3852, Val Acc: 0.6610,, LR: 0.001000\nEpoch [20/120] Loss: 19.4959, Val Acc: 0.7070,, LR: 0.001000\nEpoch [21/120] Loss: 17.9903, Val Acc: 0.7090,, LR: 0.001000\nEpoch [22/120] Loss: 17.0811, Val Acc: 0.7570,, LR: 0.001000\nEpoch [23/120] Loss: 15.9980, Val Acc: 0.7640,, LR: 0.001000\nEpoch [24/120] Loss: 15.0419, Val Acc: 0.7700,, LR: 0.001000\nEpoch [25/120] Loss: 13.8866, Val Acc: 0.7930,, LR: 0.001000\nEpoch [26/120] Loss: 13.3546, Val Acc: 0.7730,, LR: 0.001000\nEpoch [27/120] Loss: 12.7556, Val Acc: 0.7920,, LR: 0.001000\nEpoch [28/120] Loss: 11.4664, Val Acc: 0.8090,, LR: 0.001000\nEpoch [29/120] Loss: 10.6057, Val Acc: 0.7820,, LR: 0.001000\nEpoch [30/120] Loss: 10.0036, Val Acc: 0.8150,, LR: 0.001000\nEpoch [31/120] Loss: 9.3825, Val Acc: 0.7970,, LR: 0.001000\nEpoch [32/120] Loss: 8.7846, Val Acc: 0.8180,, LR: 0.001000\nEpoch [33/120] Loss: 8.2672, Val Acc: 0.8110,, LR: 0.001000\nEpoch [34/120] Loss: 7.4219, Val Acc: 0.8180,, LR: 0.001000\nEpoch [35/120] Loss: 7.4068, Val Acc: 0.8030,, LR: 0.001000\nEpoch [36/120] Loss: 6.7710, Val Acc: 0.8030,, LR: 0.001000\nEpoch [37/120] Loss: 6.0936, Val Acc: 0.8190,, LR: 0.001000\nEpoch [38/120] Loss: 5.8987, Val Acc: 0.8140,, LR: 0.001000\nEpoch [39/120] Loss: 5.5095, Val Acc: 0.8060,, LR: 0.001000\nEpoch [40/120] Loss: 5.5755, Val Acc: 0.8190,, LR: 0.001000\nEpoch [41/120] Loss: 5.7920, Val Acc: 0.8020,, LR: 0.001000\nEpoch [42/120] Loss: 4.8165, Val Acc: 0.7900,, LR: 0.001000\nEpoch [43/120] Loss: 4.0787, Val Acc: 0.8150,, LR: 0.001000\nEpoch [44/120] Loss: 3.7312, Val Acc: 0.8160,, LR: 0.001000\nEpoch [45/120] Loss: 3.5259, Val Acc: 0.8030,, LR: 0.001000\nEpoch [46/120] Loss: 3.2028, Val Acc: 0.7990,, LR: 0.001000\nEpoch [47/120] Loss: 2.7908, Val Acc: 0.7980,, LR: 0.001000\nEpoch [48/120] Loss: 2.6762, Val Acc: 0.8050,, LR: 0.001000\nEpoch [49/120] Loss: 2.5421, Val Acc: 0.8270,, LR: 0.001000\nEpoch [50/120] Loss: 2.2779, Val Acc: 0.8140,, LR: 0.001000\nEpoch [51/120] Loss: 2.0810, Val Acc: 0.8190,, LR: 0.001000\nEpoch [52/120] Loss: 2.1360, Val Acc: 0.8160,, LR: 0.001000\nEpoch [53/120] Loss: 2.0970, Val Acc: 0.8100,, LR: 0.001000\nEpoch [54/120] Loss: 2.2538, Val Acc: 0.8170,, LR: 0.001000\nEpoch [55/120] Loss: 2.0418, Val Acc: 0.8260,, LR: 0.001000\nEpoch [56/120] Loss: 1.5858, Val Acc: 0.8080,, LR: 0.001000\nEpoch [57/120] Loss: 1.3415, Val Acc: 0.8180,, LR: 0.001000\nEpoch [58/120] Loss: 1.1113, Val Acc: 0.8280,, LR: 0.001000\nEpoch [59/120] Loss: 1.0032, Val Acc: 0.8320,, LR: 0.001000\nEpoch [60/120] Loss: 0.8905, Val Acc: 0.8480,, LR: 0.001000\nEpoch [61/120] Loss: 0.7612, Val Acc: 0.8260,, LR: 0.001000\nEpoch [62/120] Loss: 0.6945, Val Acc: 0.8250,, LR: 0.001000\nEpoch [63/120] Loss: 0.5560, Val Acc: 0.8240,, LR: 0.001000\nEpoch [64/120] Loss: 0.4500, Val Acc: 0.8440,, LR: 0.001000\nEpoch [65/120] Loss: 0.3785, Val Acc: 0.8290,, LR: 0.001000\nEpoch [66/120] Loss: 0.3128, Val Acc: 0.8210,, LR: 0.001000\nEpoch [67/120] Loss: 0.2496, Val Acc: 0.8130,, LR: 0.001000\nEpoch [68/120] Loss: 0.2996, Val Acc: 0.8210,, LR: 0.001000\nEpoch [69/120] Loss: 0.2777, Val Acc: 0.8340,, LR: 0.001000\nEpoch [70/120] Loss: 0.2958, Val Acc: 0.8250,, LR: 0.001000\nEpoch [71/120] Loss: 0.2841, Val Acc: 0.8170,, LR: 0.001000\nEpoch [72/120] Loss: 0.1918, Val Acc: 0.8420,, LR: 0.001000\nEpoch [73/120] Loss: 0.1454, Val Acc: 0.8320,, LR: 0.001000\nEpoch [74/120] Loss: 0.0764, Val Acc: 0.8260,, LR: 0.000500\nEpoch [75/120] Loss: 0.0222, Val Acc: 0.8190,, LR: 0.000500\nEpoch [76/120] Loss: 0.0097, Val Acc: 0.8220,, LR: 0.000500\nEpoch [77/120] Loss: 0.0079, Val Acc: 0.8150,, LR: 0.000500\nEpoch [78/120] Loss: 0.0045, Val Acc: 0.8170,, LR: 0.000500\nEpoch [79/120] Loss: 0.0050, Val Acc: 0.8250,, LR: 0.000500\nEpoch [80/120] Loss: 0.0043, Val Acc: 0.8200,, LR: 0.000500\nEpoch [81/120] Loss: 0.0033, Val Acc: 0.8330,, LR: 0.000500\nEpoch [82/120] Loss: 0.0030, Val Acc: 0.8250,, LR: 0.000500\nEpoch [83/120] Loss: 0.0027, Val Acc: 0.8200,, LR: 0.000500\nEpoch [84/120] Loss: 0.0026, Val Acc: 0.8170,, LR: 0.000500\nEpoch [85/120] Loss: 0.0026, Val Acc: 0.8220,, LR: 0.000500\nEpoch [86/120] Loss: 0.0024, Val Acc: 0.8250,, LR: 0.000500\nEpoch [87/120] Loss: 0.0024, Val Acc: 0.8200,, LR: 0.000250\nEpoch [88/120] Loss: 0.0023, Val Acc: 0.8100,, LR: 0.000250\nEpoch [89/120] Loss: 0.0022, Val Acc: 0.8330,, LR: 0.000250\nEpoch [90/120] Loss: 0.0021, Val Acc: 0.8210,, LR: 0.000250\nEpoch [91/120] Loss: 0.0021, Val Acc: 0.8250,, LR: 0.000250\nEpoch [92/120] Loss: 0.0021, Val Acc: 0.8240,, LR: 0.000250\nEpoch [93/120] Loss: 0.0020, Val Acc: 0.8150,, LR: 0.000250\nEpoch [94/120] Loss: 0.0020, Val Acc: 0.8330,, LR: 0.000250\nEpoch [95/120] Loss: 0.0019, Val Acc: 0.8290,, LR: 0.000250\nEpoch [96/120] Loss: 0.0019, Val Acc: 0.8260,, LR: 0.000250\nEpoch [97/120] Loss: 0.0019, Val Acc: 0.8190,, LR: 0.000250\nEpoch [98/120] Loss: 0.0019, Val Acc: 0.8260,, LR: 0.000250\nEpoch [99/120] Loss: 0.0019, Val Acc: 0.8230,, LR: 0.000250\nEpoch [100/120] Loss: 0.0019, Val Acc: 0.8360,, LR: 0.000125\nEpoch [101/120] Loss: 0.0019, Val Acc: 0.8330,, LR: 0.000125\nEpoch [102/120] Loss: 0.0018, Val Acc: 0.8240,, LR: 0.000125\nEpoch [103/120] Loss: 0.0018, Val Acc: 0.8210,, LR: 0.000125\nEpoch [104/120] Loss: 0.0018, Val Acc: 0.8230,, LR: 0.000125\nEpoch [105/120] Loss: 0.0018, Val Acc: 0.8250,, LR: 0.000125\nEpoch [106/120] Loss: 0.0018, Val Acc: 0.8200,, LR: 0.000125\nEpoch [107/120] Loss: 0.0018, Val Acc: 0.8300,, LR: 0.000125\nEpoch [108/120] Loss: 0.0017, Val Acc: 0.8120,, LR: 0.000125\nEpoch [109/120] Loss: 0.0018, Val Acc: 0.8250,, LR: 0.000125\nEpoch [110/120] Loss: 0.0018, Val Acc: 0.8250,, LR: 0.000125\nEpoch [111/120] Loss: 0.0017, Val Acc: 0.8240,, LR: 0.000125\nEpoch [112/120] Loss: 0.0017, Val Acc: 0.8380,, LR: 0.000125\nEpoch [113/120] Loss: 0.0017, Val Acc: 0.8310,, LR: 0.000063\nEpoch [114/120] Loss: 0.0017, Val Acc: 0.8330,, LR: 0.000063\nEpoch [115/120] Loss: 0.0017, Val Acc: 0.8310,, LR: 0.000063\nEpoch [116/120] Loss: 0.0017, Val Acc: 0.8360,, LR: 0.000063\nEpoch [117/120] Loss: 0.0017, Val Acc: 0.8260,, LR: 0.000063\nEpoch [118/120] Loss: 0.0017, Val Acc: 0.8270,, LR: 0.000063\nEpoch [119/120] Loss: 0.0017, Val Acc: 0.8340,, LR: 0.000063\nEpoch [120/120] Loss: 0.0017, Val Acc: 0.8270,, LR: 0.000063\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"test(trainer.model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T04:06:36.767554Z","iopub.execute_input":"2025-05-25T04:06:36.767854Z","iopub.status.idle":"2025-05-25T04:06:38.837653Z","shell.execute_reply.started":"2025-05-25T04:06:36.767830Z","shell.execute_reply":"2025-05-25T04:06:38.836785Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"0.837"},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"## CosFace Loss Model with Test Acc: 83.7%","metadata":{}},{"cell_type":"code","source":"model_name='edgeface_s_gamma_05'\nembedding_size=512\nmargin_list = (1.0, 0.25, 0.0)\nnum_classes = len(dataset.classes)\nsample_rate = 1\nlr = 1e-3\nweight_decay = 0.05","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:08:17.310088Z","iopub.execute_input":"2025-05-25T05:08:17.310388Z","iopub.status.idle":"2025-05-25T05:08:17.314889Z","shell.execute_reply.started":"2025-05-25T05:08:17.310361Z","shell.execute_reply":"2025-05-25T05:08:17.314193Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"trainer=LFWTrainer(model_name = model_name, \n                    embedding_size = embedding_size, \n                    train_loader = train_loader, \n                     val_loader = val_loader,\n                      margin_list= margin_list\n                  )\n\ntrainer.train(num_epochs=120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:12:52.593275Z","iopub.execute_input":"2025-05-25T05:12:52.593531Z","iopub.status.idle":"2025-05-25T05:28:14.061632Z","shell.execute_reply.started":"2025-05-25T05:12:52.593514Z","shell.execute_reply":"2025-05-25T05:28:14.060725Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/120] Loss: 23.6632, Val Acc: 0.5000,, LR: 0.001000\nEpoch [2/120] Loss: 21.4760, Val Acc: 0.5040,, LR: 0.001000\nEpoch [3/120] Loss: 20.9318, Val Acc: 0.5070,, LR: 0.001000\nEpoch [4/120] Loss: 20.7911, Val Acc: 0.5260,, LR: 0.001000\nEpoch [5/120] Loss: 20.6231, Val Acc: 0.5260,, LR: 0.001000\nEpoch [6/120] Loss: 20.2912, Val Acc: 0.5750,, LR: 0.001000\nEpoch [7/120] Loss: 20.0190, Val Acc: 0.5870,, LR: 0.001000\nEpoch [8/120] Loss: 19.6621, Val Acc: 0.5990,, LR: 0.001000\nEpoch [9/120] Loss: 19.3801, Val Acc: 0.6380,, LR: 0.001000\nEpoch [10/120] Loss: 18.8653, Val Acc: 0.6200,, LR: 0.001000\nEpoch [11/120] Loss: 17.9028, Val Acc: 0.6190,, LR: 0.001000\nEpoch [12/120] Loss: 16.8983, Val Acc: 0.6170,, LR: 0.001000\nEpoch [13/120] Loss: 15.9060, Val Acc: 0.6470,, LR: 0.001000\nEpoch [14/120] Loss: 15.2050, Val Acc: 0.6520,, LR: 0.001000\nEpoch [15/120] Loss: 14.4358, Val Acc: 0.6760,, LR: 0.001000\nEpoch [16/120] Loss: 13.5192, Val Acc: 0.6800,, LR: 0.001000\nEpoch [17/120] Loss: 12.2151, Val Acc: 0.7570,, LR: 0.001000\nEpoch [18/120] Loss: 11.1717, Val Acc: 0.7600,, LR: 0.001000\nEpoch [19/120] Loss: 9.6207, Val Acc: 0.7710,, LR: 0.001000\nEpoch [20/120] Loss: 8.7709, Val Acc: 0.7840,, LR: 0.001000\nEpoch [21/120] Loss: 7.6230, Val Acc: 0.7770,, LR: 0.001000\nEpoch [22/120] Loss: 6.6787, Val Acc: 0.8030,, LR: 0.001000\nEpoch [23/120] Loss: 5.8747, Val Acc: 0.8050,, LR: 0.001000\nEpoch [24/120] Loss: 5.1593, Val Acc: 0.7800,, LR: 0.001000\nEpoch [25/120] Loss: 4.3617, Val Acc: 0.8190,, LR: 0.001000\nEpoch [26/120] Loss: 3.9268, Val Acc: 0.8190,, LR: 0.001000\nEpoch [27/120] Loss: 3.6569, Val Acc: 0.8150,, LR: 0.001000\nEpoch [28/120] Loss: 3.1038, Val Acc: 0.8210,, LR: 0.001000\nEpoch [29/120] Loss: 2.7022, Val Acc: 0.8380,, LR: 0.001000\nEpoch [30/120] Loss: 2.4216, Val Acc: 0.8080,, LR: 0.001000\nEpoch [31/120] Loss: 2.0594, Val Acc: 0.8290,, LR: 0.001000\nEpoch [32/120] Loss: 1.7693, Val Acc: 0.8330,, LR: 0.001000\nEpoch [33/120] Loss: 1.6501, Val Acc: 0.8190,, LR: 0.001000\nEpoch [34/120] Loss: 1.5214, Val Acc: 0.8190,, LR: 0.001000\nEpoch [35/120] Loss: 1.2232, Val Acc: 0.8370,, LR: 0.001000\nEpoch [36/120] Loss: 1.0935, Val Acc: 0.8420,, LR: 0.001000\nEpoch [37/120] Loss: 1.0120, Val Acc: 0.8130,, LR: 0.001000\nEpoch [38/120] Loss: 0.9472, Val Acc: 0.8330,, LR: 0.001000\nEpoch [39/120] Loss: 0.6425, Val Acc: 0.8350,, LR: 0.001000\nEpoch [40/120] Loss: 0.5803, Val Acc: 0.8240,, LR: 0.001000\nEpoch [41/120] Loss: 0.4270, Val Acc: 0.8430,, LR: 0.001000\nEpoch [42/120] Loss: 0.3731, Val Acc: 0.8410,, LR: 0.001000\nEpoch [43/120] Loss: 0.3168, Val Acc: 0.8270,, LR: 0.001000\nEpoch [44/120] Loss: 0.2884, Val Acc: 0.8330,, LR: 0.001000\nEpoch [45/120] Loss: 0.2704, Val Acc: 0.8300,, LR: 0.001000\nEpoch [46/120] Loss: 0.2239, Val Acc: 0.8450,, LR: 0.001000\nEpoch [47/120] Loss: 0.2247, Val Acc: 0.8290,, LR: 0.001000\nEpoch [48/120] Loss: 0.1967, Val Acc: 0.8250,, LR: 0.001000\nEpoch [49/120] Loss: 0.1449, Val Acc: 0.8230,, LR: 0.001000\nEpoch [50/120] Loss: 0.1168, Val Acc: 0.8210,, LR: 0.001000\nEpoch [51/120] Loss: 0.1239, Val Acc: 0.8180,, LR: 0.001000\nEpoch [52/120] Loss: 0.1473, Val Acc: 0.8250,, LR: 0.001000\nEpoch [53/120] Loss: 0.1604, Val Acc: 0.8400,, LR: 0.001000\nEpoch [54/120] Loss: 0.1228, Val Acc: 0.8330,, LR: 0.001000\nEpoch [55/120] Loss: 0.0990, Val Acc: 0.8310,, LR: 0.001000\nEpoch [56/120] Loss: 0.0986, Val Acc: 0.8410,, LR: 0.001000\nEpoch [57/120] Loss: 0.0730, Val Acc: 0.8300,, LR: 0.001000\nEpoch [58/120] Loss: 0.0781, Val Acc: 0.8300,, LR: 0.001000\nEpoch [59/120] Loss: 0.2155, Val Acc: 0.8270,, LR: 0.001000\nEpoch [60/120] Loss: 0.0979, Val Acc: 0.8390,, LR: 0.000500\nEpoch [61/120] Loss: 0.0319, Val Acc: 0.8360,, LR: 0.000500\nEpoch [62/120] Loss: 0.0125, Val Acc: 0.8350,, LR: 0.000500\nEpoch [63/120] Loss: 0.0065, Val Acc: 0.8440,, LR: 0.000500\nEpoch [64/120] Loss: 0.0019, Val Acc: 0.8310,, LR: 0.000500\nEpoch [65/120] Loss: 0.0011, Val Acc: 0.8380,, LR: 0.000500\nEpoch [66/120] Loss: 0.0008, Val Acc: 0.8450,, LR: 0.000500\nEpoch [67/120] Loss: 0.0007, Val Acc: 0.8320,, LR: 0.000500\nEpoch [68/120] Loss: 0.0008, Val Acc: 0.8300,, LR: 0.000500\nEpoch [69/120] Loss: 0.0007, Val Acc: 0.8350,, LR: 0.000500\nEpoch [70/120] Loss: 0.0006, Val Acc: 0.8300,, LR: 0.000500\nEpoch [71/120] Loss: 0.0005, Val Acc: 0.8390,, LR: 0.000500\nEpoch [72/120] Loss: 0.0005, Val Acc: 0.8390,, LR: 0.000500\nEpoch [73/120] Loss: 0.0005, Val Acc: 0.8340,, LR: 0.000250\nEpoch [74/120] Loss: 0.0005, Val Acc: 0.8360,, LR: 0.000250\nEpoch [75/120] Loss: 0.0005, Val Acc: 0.8360,, LR: 0.000250\nEpoch [76/120] Loss: 0.0005, Val Acc: 0.8340,, LR: 0.000250\nEpoch [77/120] Loss: 0.0005, Val Acc: 0.8320,, LR: 0.000250\nEpoch [78/120] Loss: 0.0005, Val Acc: 0.8380,, LR: 0.000250\nEpoch [79/120] Loss: 0.0005, Val Acc: 0.8380,, LR: 0.000250\nEpoch [80/120] Loss: 0.0005, Val Acc: 0.8380,, LR: 0.000250\nEpoch [81/120] Loss: 0.0004, Val Acc: 0.8320,, LR: 0.000250\nEpoch [82/120] Loss: 0.0004, Val Acc: 0.8370,, LR: 0.000250\nEpoch [83/120] Loss: 0.0004, Val Acc: 0.8340,, LR: 0.000250\nEpoch [84/120] Loss: 0.0004, Val Acc: 0.8470,, LR: 0.000250\nEpoch [85/120] Loss: 0.0004, Val Acc: 0.8410,, LR: 0.000250\nEpoch [86/120] Loss: 0.0004, Val Acc: 0.8330,, LR: 0.000250\nEpoch [87/120] Loss: 0.0004, Val Acc: 0.8370,, LR: 0.000250\nEpoch [88/120] Loss: 0.0004, Val Acc: 0.8420,, LR: 0.000250\nEpoch [89/120] Loss: 0.0004, Val Acc: 0.8410,, LR: 0.000250\nEpoch [90/120] Loss: 0.0004, Val Acc: 0.8430,, LR: 0.000250\nEpoch [91/120] Loss: 0.0004, Val Acc: 0.8410,, LR: 0.000250\nEpoch [92/120] Loss: 0.0004, Val Acc: 0.8400,, LR: 0.000250\nEpoch [93/120] Loss: 0.0004, Val Acc: 0.8400,, LR: 0.000250\nEpoch [94/120] Loss: 0.0004, Val Acc: 0.8310,, LR: 0.000250\nEpoch [95/120] Loss: 0.0004, Val Acc: 0.8360,, LR: 0.000250\nEpoch [96/120] Loss: 0.0004, Val Acc: 0.8350,, LR: 0.000250\nEpoch [97/120] Loss: 0.0004, Val Acc: 0.8390,, LR: 0.000250\nEpoch [98/120] Loss: 0.0004, Val Acc: 0.8360,, LR: 0.000125\nEpoch [99/120] Loss: 0.0003, Val Acc: 0.8360,, LR: 0.000125\nEpoch [100/120] Loss: 0.0004, Val Acc: 0.8390,, LR: 0.000125\nEpoch [101/120] Loss: 0.0003, Val Acc: 0.8350,, LR: 0.000125\nEpoch [102/120] Loss: 0.0003, Val Acc: 0.8360,, LR: 0.000125\nEpoch [103/120] Loss: 0.0003, Val Acc: 0.8380,, LR: 0.000125\nEpoch [104/120] Loss: 0.0003, Val Acc: 0.8370,, LR: 0.000125\nEpoch [105/120] Loss: 0.0003, Val Acc: 0.8320,, LR: 0.000125\nEpoch [106/120] Loss: 0.0003, Val Acc: 0.8350,, LR: 0.000125\nEpoch [107/120] Loss: 0.0003, Val Acc: 0.8390,, LR: 0.000125\nEpoch [108/120] Loss: 0.0003, Val Acc: 0.8340,, LR: 0.000125\nEpoch [109/120] Loss: 0.0003, Val Acc: 0.8440,, LR: 0.000125\nEpoch [110/120] Loss: 0.0003, Val Acc: 0.8390,, LR: 0.000125\nEpoch [111/120] Loss: 0.0003, Val Acc: 0.8360,, LR: 0.000063\nEpoch [112/120] Loss: 0.0003, Val Acc: 0.8420,, LR: 0.000063\nEpoch [113/120] Loss: 0.0003, Val Acc: 0.8370,, LR: 0.000063\nEpoch [114/120] Loss: 0.0003, Val Acc: 0.8370,, LR: 0.000063\nEpoch [115/120] Loss: 0.0003, Val Acc: 0.8370,, LR: 0.000063\nEpoch [116/120] Loss: 0.0003, Val Acc: 0.8330,, LR: 0.000063\nEpoch [117/120] Loss: 0.0003, Val Acc: 0.8300,, LR: 0.000063\nEpoch [118/120] Loss: 0.0003, Val Acc: 0.8420,, LR: 0.000063\nEpoch [119/120] Loss: 0.0003, Val Acc: 0.8370,, LR: 0.000063\nEpoch [120/120] Loss: 0.0003, Val Acc: 0.8370,, LR: 0.000063\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"test(trainer.model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:39:42.188539Z","iopub.execute_input":"2025-05-25T05:39:42.188876Z","iopub.status.idle":"2025-05-25T05:39:44.298473Z","shell.execute_reply.started":"2025-05-25T05:39:42.188850Z","shell.execute_reply":"2025-05-25T05:39:44.297519Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"0.844"},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"## ArcFace Loss Model with Test Acc: 84.4%","metadata":{}},{"cell_type":"code","source":"model_name='edgeface_s_gamma_05'\nembedding_size=512\nmargin_list = (1.5, 0.0, 0.0)\nnum_classes = len(dataset.classes)\nsample_rate = 1\nlr = 1e-3\nweight_decay = 0.05","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:08:18.666983Z","iopub.execute_input":"2025-05-25T06:08:18.667657Z","iopub.status.idle":"2025-05-25T06:08:18.671790Z","shell.execute_reply.started":"2025-05-25T06:08:18.667628Z","shell.execute_reply":"2025-05-25T06:08:18.671237Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"trainer=LFWTrainer(model_name = model_name, \n                    embedding_size = embedding_size, \n                    train_loader = train_loader, \n                     val_loader = val_loader,\n                      margin_list= margin_list\n                  )\n\ntrainer.train(num_epochs=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:14:00.210521Z","iopub.execute_input":"2025-05-25T06:14:00.211284Z","iopub.status.idle":"2025-05-25T06:26:56.677093Z","shell.execute_reply.started":"2025-05-25T06:14:00.211222Z","shell.execute_reply":"2025-05-25T06:26:56.676333Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100] Loss: 51.5502, Val Acc: 0.5030,, LR: 0.001000\nEpoch [2/100] Loss: 41.1902, Val Acc: 0.5050,, LR: 0.001000\nEpoch [3/100] Loss: 21.3510, Val Acc: 0.5020,, LR: 0.001000\nEpoch [4/100] Loss: 8.7982, Val Acc: 0.5000,, LR: 0.001000\nEpoch [5/100] Loss: 5.8497, Val Acc: 0.5000,, LR: 0.001000\nEpoch [6/100] Loss: 5.2681, Val Acc: 0.5000,, LR: 0.001000\nEpoch [7/100] Loss: 5.1279, Val Acc: 0.5000,, LR: 0.001000\nEpoch [8/100] Loss: 5.0865, Val Acc: 0.5000,, LR: 0.001000\nEpoch [9/100] Loss: 5.0703, Val Acc: 0.5000,, LR: 0.001000\nEpoch [10/100] Loss: 5.0623, Val Acc: 0.5000,, LR: 0.001000\nEpoch [11/100] Loss: 5.0612, Val Acc: 0.5000,, LR: 0.001000\nEpoch [12/100] Loss: 5.0547, Val Acc: 0.5000,, LR: 0.001000\nEpoch [13/100] Loss: 5.0336, Val Acc: 0.5250,, LR: 0.001000\nEpoch [14/100] Loss: 5.0123, Val Acc: 0.5280,, LR: 0.001000\nEpoch [15/100] Loss: 4.9940, Val Acc: 0.5580,, LR: 0.001000\nEpoch [16/100] Loss: 4.9248, Val Acc: 0.5680,, LR: 0.001000\nEpoch [17/100] Loss: 4.9039, Val Acc: 0.6200,, LR: 0.001000\nEpoch [18/100] Loss: 4.8604, Val Acc: 0.6220,, LR: 0.001000\nEpoch [19/100] Loss: 4.8883, Val Acc: 0.6110,, LR: 0.001000\nEpoch [20/100] Loss: 4.8420, Val Acc: 0.6240,, LR: 0.001000\nEpoch [21/100] Loss: 4.7795, Val Acc: 0.6510,, LR: 0.001000\nEpoch [22/100] Loss: 4.7499, Val Acc: 0.6320,, LR: 0.001000\nEpoch [23/100] Loss: 4.7131, Val Acc: 0.6270,, LR: 0.001000\nEpoch [24/100] Loss: 4.7730, Val Acc: 0.6400,, LR: 0.001000\nEpoch [25/100] Loss: 4.7009, Val Acc: 0.6540,, LR: 0.001000\nEpoch [26/100] Loss: 4.6461, Val Acc: 0.6680,, LR: 0.001000\nEpoch [27/100] Loss: 4.6251, Val Acc: 0.6820,, LR: 0.001000\nEpoch [28/100] Loss: 4.5671, Val Acc: 0.6640,, LR: 0.001000\nEpoch [29/100] Loss: 4.5523, Val Acc: 0.6840,, LR: 0.001000\nEpoch [30/100] Loss: 4.5930, Val Acc: 0.6970,, LR: 0.001000\nEpoch [31/100] Loss: 4.5121, Val Acc: 0.6840,, LR: 0.001000\nEpoch [32/100] Loss: 4.5265, Val Acc: 0.7090,, LR: 0.001000\nEpoch [33/100] Loss: 4.4238, Val Acc: 0.7140,, LR: 0.001000\nEpoch [34/100] Loss: 4.3719, Val Acc: 0.7170,, LR: 0.001000\nEpoch [35/100] Loss: 4.3670, Val Acc: 0.7110,, LR: 0.001000\nEpoch [36/100] Loss: 4.3897, Val Acc: 0.7190,, LR: 0.001000\nEpoch [37/100] Loss: 4.2369, Val Acc: 0.7290,, LR: 0.001000\nEpoch [38/100] Loss: 4.1561, Val Acc: 0.7220,, LR: 0.001000\nEpoch [39/100] Loss: 4.1995, Val Acc: 0.7330,, LR: 0.001000\nEpoch [40/100] Loss: 4.1145, Val Acc: 0.7410,, LR: 0.001000\nEpoch [41/100] Loss: 4.2415, Val Acc: 0.7440,, LR: 0.001000\nEpoch [42/100] Loss: 4.0118, Val Acc: 0.7560,, LR: 0.001000\nEpoch [43/100] Loss: 3.9563, Val Acc: 0.7520,, LR: 0.001000\nEpoch [44/100] Loss: 3.8409, Val Acc: 0.7740,, LR: 0.001000\nEpoch [45/100] Loss: 3.8230, Val Acc: 0.7360,, LR: 0.001000\nEpoch [46/100] Loss: 3.8922, Val Acc: 0.7670,, LR: 0.001000\nEpoch [47/100] Loss: 3.6286, Val Acc: 0.7470,, LR: 0.001000\nEpoch [48/100] Loss: 3.5675, Val Acc: 0.7740,, LR: 0.001000\nEpoch [49/100] Loss: 3.6195, Val Acc: 0.7600,, LR: 0.001000\nEpoch [50/100] Loss: 3.5349, Val Acc: 0.7680,, LR: 0.001000\nEpoch [51/100] Loss: 3.3939, Val Acc: 0.7680,, LR: 0.001000\nEpoch [52/100] Loss: 3.3797, Val Acc: 0.7660,, LR: 0.001000\nEpoch [53/100] Loss: 3.2613, Val Acc: 0.7700,, LR: 0.001000\nEpoch [54/100] Loss: 3.2692, Val Acc: 0.7640,, LR: 0.001000\nEpoch [55/100] Loss: 3.1138, Val Acc: 0.7830,, LR: 0.001000\nEpoch [56/100] Loss: 3.0505, Val Acc: 0.7870,, LR: 0.001000\nEpoch [57/100] Loss: 3.0251, Val Acc: 0.7890,, LR: 0.001000\nEpoch [58/100] Loss: 2.9644, Val Acc: 0.7810,, LR: 0.001000\nEpoch [59/100] Loss: 2.9548, Val Acc: 0.7850,, LR: 0.001000\nEpoch [60/100] Loss: 2.9128, Val Acc: 0.7640,, LR: 0.001000\nEpoch [61/100] Loss: 2.8704, Val Acc: 0.7930,, LR: 0.001000\nEpoch [62/100] Loss: 2.7306, Val Acc: 0.7870,, LR: 0.001000\nEpoch [63/100] Loss: 2.8094, Val Acc: 0.7840,, LR: 0.001000\nEpoch [64/100] Loss: 2.7673, Val Acc: 0.7910,, LR: 0.001000\nEpoch [65/100] Loss: 2.6030, Val Acc: 0.7850,, LR: 0.001000\nEpoch [66/100] Loss: 2.5213, Val Acc: 0.7800,, LR: 0.001000\nEpoch [67/100] Loss: 2.4783, Val Acc: 0.7750,, LR: 0.001000\nEpoch [68/100] Loss: 2.4400, Val Acc: 0.8120,, LR: 0.001000\nEpoch [69/100] Loss: 2.2733, Val Acc: 0.7890,, LR: 0.001000\nEpoch [70/100] Loss: 2.4021, Val Acc: 0.8010,, LR: 0.001000\nEpoch [71/100] Loss: 2.3877, Val Acc: 0.7930,, LR: 0.001000\nEpoch [72/100] Loss: 2.2412, Val Acc: 0.8050,, LR: 0.001000\nEpoch [73/100] Loss: 2.1777, Val Acc: 0.8050,, LR: 0.001000\nEpoch [74/100] Loss: 2.0158, Val Acc: 0.7940,, LR: 0.001000\nEpoch [75/100] Loss: 2.0489, Val Acc: 0.8020,, LR: 0.001000\nEpoch [76/100] Loss: 1.8817, Val Acc: 0.8040,, LR: 0.001000\nEpoch [77/100] Loss: 1.8505, Val Acc: 0.7970,, LR: 0.001000\nEpoch [78/100] Loss: 1.9014, Val Acc: 0.8030,, LR: 0.001000\nEpoch [79/100] Loss: 1.8568, Val Acc: 0.8020,, LR: 0.001000\nEpoch [80/100] Loss: 1.7676, Val Acc: 0.7950,, LR: 0.001000\nEpoch [81/100] Loss: 1.5974, Val Acc: 0.8170,, LR: 0.001000\nEpoch [82/100] Loss: 1.6228, Val Acc: 0.8220,, LR: 0.001000\nEpoch [83/100] Loss: 1.6367, Val Acc: 0.8120,, LR: 0.001000\nEpoch [84/100] Loss: 1.4862, Val Acc: 0.8150,, LR: 0.001000\nEpoch [85/100] Loss: 1.3346, Val Acc: 0.8090,, LR: 0.001000\nEpoch [86/100] Loss: 1.3492, Val Acc: 0.8180,, LR: 0.001000\nEpoch [87/100] Loss: 1.3314, Val Acc: 0.8110,, LR: 0.001000\nEpoch [88/100] Loss: 1.3903, Val Acc: 0.8230,, LR: 0.001000\nEpoch [89/100] Loss: 1.2963, Val Acc: 0.8200,, LR: 0.001000\nEpoch [90/100] Loss: 1.3181, Val Acc: 0.8200,, LR: 0.001000\nEpoch [91/100] Loss: 1.2355, Val Acc: 0.8110,, LR: 0.001000\nEpoch [92/100] Loss: 1.1642, Val Acc: 0.8000,, LR: 0.001000\nEpoch [93/100] Loss: 1.0406, Val Acc: 0.8330,, LR: 0.001000\nEpoch [94/100] Loss: 1.0031, Val Acc: 0.8170,, LR: 0.001000\nEpoch [95/100] Loss: 1.1349, Val Acc: 0.8190,, LR: 0.001000\nEpoch [96/100] Loss: 1.0506, Val Acc: 0.8220,, LR: 0.001000\nEpoch [97/100] Loss: 1.0242, Val Acc: 0.8250,, LR: 0.001000\nEpoch [98/100] Loss: 0.8374, Val Acc: 0.8310,, LR: 0.001000\nEpoch [99/100] Loss: 0.7083, Val Acc: 0.8280,, LR: 0.001000\nEpoch [100/100] Loss: 0.6228, Val Acc: 0.8230,, LR: 0.001000\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"test(trainer.model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:32:01.418801Z","iopub.execute_input":"2025-05-25T06:32:01.419617Z","iopub.status.idle":"2025-05-25T06:32:03.595821Z","shell.execute_reply.started":"2025-05-25T06:32:01.419586Z","shell.execute_reply":"2025-05-25T06:32:03.595060Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"0.812"},"metadata":{}}],"execution_count":84},{"cell_type":"markdown","source":"## SphereFace Loss Model with Test Acc: 81.2%","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom typing import Callable\n\n\nclass LFWTripletTrainer:\n    def __init__(self, model_name, embedding_size,\n                 train_loader, val_loader,\n                 lr=0.001, weight_decay=5e-4, margin=1):\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Model (embedding-only)\n        self.model = get_model(model_name, dropout=0.0, num_features=embedding_size)\n        self.model = self.model.to(self.device)\n\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        # Triplet Loss\n        self.criterion = nn.TripletMarginLoss(margin=margin, p=2)\n\n        # Optimizer\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=20, verbose=True)\n\n        # Transform\n        self.transform = transforms.Compose([\n            transforms.Resize((112, 112)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n    def train(self, num_epochs=10, save_path='checkpoints'):\n        os.makedirs(save_path, exist_ok=True)\n\n        for epoch in range(num_epochs):\n            self.model.train()\n            running_loss = 0.0\n\n            for i, (anchor, positive, negative) in enumerate(self.train_loader):\n                anchor = anchor.to(self.device)\n                positive = positive.to(self.device)\n                negative = negative.to(self.device)\n\n                anchor_emb = F.normalize(self.model(anchor))\n                positive_emb = F.normalize(self.model(positive))\n                negative_emb = F.normalize(self.model(negative))\n\n                loss = self.criterion(anchor_emb, positive_emb, negative_emb)\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss += loss.item()\n\n                # if i % 10 == 0:\n                #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(self.train_loader)}], Loss: {loss.item():.4f}')\n\n            val_acc = self.validate()\n            current_lr = self.optimizer.param_groups[0]['lr']\n            epoch_loss = running_loss / len(self.train_loader)\n            print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f},'\n            f', LR: {current_lr:.6f}')\n\n            self.scheduler.step(val_acc)\n            \n    def validate(self):\n        self.model.eval()\n        all_sims = []\n        all_labels = []\n    \n        with torch.no_grad():\n            for img1, img2, label in self.val_loader:\n                img1, img2 = img1.to(self.device), img2.to(self.device)\n                label = label.to(self.device)\n    \n                emb1 = F.normalize(self.model(img1))\n                emb2 = F.normalize(self.model(img2))\n                sim = F.cosine_similarity(emb1, emb2)\n    \n                all_sims.extend(sim.cpu().numpy())\n                all_labels.extend(label.cpu().numpy())\n    \n        all_sims = np.array(all_sims)\n        all_labels = np.array(all_labels)\n    \n        # Find the best threshold\n        best_acc = 0.0\n        best_thresh = 0.0\n        for thresh in np.arange(0, 1.01, 0.01):\n            preds = (all_sims > thresh).astype(int)\n            acc = (preds == all_labels).mean()\n            if acc > best_acc:\n                best_acc = acc\n                best_thresh = thresh\n        return best_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T04:09:47.074149Z","iopub.execute_input":"2025-05-25T04:09:47.074831Z","iopub.status.idle":"2025-05-25T04:09:47.087619Z","shell.execute_reply.started":"2025-05-25T04:09:47.074805Z","shell.execute_reply":"2025-05-25T04:09:47.087012Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"triplet_trainer= LFWTripletTrainer(\n        model_name= model_name,\n        embedding_size=embedding_size,\n        train_loader=triplet_train_loader,\n        val_loader=triplet_val_loader,\n        lr=lr,\n        weight_decay=weight_decay,\n        margin = 1.5\n)\n\ntriplet_trainer.train(num_epochs=120)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T04:10:35.777753Z","iopub.execute_input":"2025-05-25T04:10:35.778219Z","iopub.status.idle":"2025-05-25T04:47:08.732223Z","shell.execute_reply.started":"2025-05-25T04:10:35.778195Z","shell.execute_reply":"2025-05-25T04:47:08.731240Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/120] Loss: 1.4538, Val Acc: 0.5000,, LR: 0.001000\nEpoch [2/120] Loss: 1.3598, Val Acc: 0.5390,, LR: 0.001000\nEpoch [3/120] Loss: 1.2776, Val Acc: 0.5870,, LR: 0.001000\nEpoch [4/120] Loss: 1.2408, Val Acc: 0.6130,, LR: 0.001000\nEpoch [5/120] Loss: 1.2192, Val Acc: 0.6090,, LR: 0.001000\nEpoch [6/120] Loss: 1.1745, Val Acc: 0.6400,, LR: 0.001000\nEpoch [7/120] Loss: 1.1742, Val Acc: 0.6380,, LR: 0.001000\nEpoch [8/120] Loss: 1.2008, Val Acc: 0.6400,, LR: 0.001000\nEpoch [9/120] Loss: 1.1283, Val Acc: 0.6460,, LR: 0.001000\nEpoch [10/120] Loss: 1.1372, Val Acc: 0.6430,, LR: 0.001000\nEpoch [11/120] Loss: 1.1899, Val Acc: 0.6200,, LR: 0.001000\nEpoch [12/120] Loss: 1.0933, Val Acc: 0.6680,, LR: 0.001000\nEpoch [13/120] Loss: 1.0439, Val Acc: 0.6330,, LR: 0.001000\nEpoch [14/120] Loss: 1.0810, Val Acc: 0.6530,, LR: 0.001000\nEpoch [15/120] Loss: 1.0521, Val Acc: 0.6450,, LR: 0.001000\nEpoch [16/120] Loss: 1.0323, Val Acc: 0.6610,, LR: 0.001000\nEpoch [17/120] Loss: 0.9957, Val Acc: 0.6520,, LR: 0.001000\nEpoch [18/120] Loss: 1.0040, Val Acc: 0.6520,, LR: 0.001000\nEpoch [19/120] Loss: 0.9984, Val Acc: 0.6600,, LR: 0.001000\nEpoch [20/120] Loss: 1.0013, Val Acc: 0.6730,, LR: 0.001000\nEpoch [21/120] Loss: 1.0049, Val Acc: 0.6450,, LR: 0.001000\nEpoch [22/120] Loss: 0.9925, Val Acc: 0.6780,, LR: 0.001000\nEpoch [23/120] Loss: 0.9797, Val Acc: 0.6580,, LR: 0.001000\nEpoch [24/120] Loss: 0.9647, Val Acc: 0.6940,, LR: 0.001000\nEpoch [25/120] Loss: 0.9467, Val Acc: 0.6750,, LR: 0.001000\nEpoch [26/120] Loss: 0.9072, Val Acc: 0.6850,, LR: 0.001000\nEpoch [27/120] Loss: 0.9449, Val Acc: 0.6910,, LR: 0.001000\nEpoch [28/120] Loss: 0.9049, Val Acc: 0.6720,, LR: 0.001000\nEpoch [29/120] Loss: 0.8944, Val Acc: 0.7060,, LR: 0.001000\nEpoch [30/120] Loss: 0.9025, Val Acc: 0.6680,, LR: 0.001000\nEpoch [31/120] Loss: 0.9051, Val Acc: 0.6740,, LR: 0.001000\nEpoch [32/120] Loss: 0.9119, Val Acc: 0.6910,, LR: 0.001000\nEpoch [33/120] Loss: 0.8848, Val Acc: 0.7070,, LR: 0.001000\nEpoch [34/120] Loss: 0.8769, Val Acc: 0.7090,, LR: 0.001000\nEpoch [35/120] Loss: 0.8682, Val Acc: 0.7000,, LR: 0.001000\nEpoch [36/120] Loss: 0.8293, Val Acc: 0.6890,, LR: 0.001000\nEpoch [37/120] Loss: 0.8692, Val Acc: 0.6770,, LR: 0.001000\nEpoch [38/120] Loss: 0.8551, Val Acc: 0.7080,, LR: 0.001000\nEpoch [39/120] Loss: 0.8723, Val Acc: 0.7020,, LR: 0.001000\nEpoch [40/120] Loss: 0.8256, Val Acc: 0.6900,, LR: 0.001000\nEpoch [41/120] Loss: 0.8322, Val Acc: 0.6840,, LR: 0.001000\nEpoch [42/120] Loss: 0.8411, Val Acc: 0.6910,, LR: 0.001000\nEpoch [43/120] Loss: 0.8089, Val Acc: 0.6960,, LR: 0.001000\nEpoch [44/120] Loss: 0.8549, Val Acc: 0.6760,, LR: 0.001000\nEpoch [45/120] Loss: 0.8418, Val Acc: 0.6830,, LR: 0.001000\nEpoch [46/120] Loss: 0.8857, Val Acc: 0.7080,, LR: 0.001000\nEpoch [47/120] Loss: 0.8391, Val Acc: 0.6840,, LR: 0.001000\nEpoch [48/120] Loss: 0.8140, Val Acc: 0.7060,, LR: 0.001000\nEpoch [49/120] Loss: 0.8273, Val Acc: 0.7090,, LR: 0.001000\nEpoch [50/120] Loss: 0.7997, Val Acc: 0.7010,, LR: 0.001000\nEpoch [51/120] Loss: 0.7989, Val Acc: 0.7040,, LR: 0.001000\nEpoch [52/120] Loss: 0.7608, Val Acc: 0.6880,, LR: 0.001000\nEpoch [53/120] Loss: 0.7815, Val Acc: 0.6990,, LR: 0.001000\nEpoch [54/120] Loss: 0.7915, Val Acc: 0.6860,, LR: 0.001000\nEpoch [55/120] Loss: 0.7503, Val Acc: 0.6900,, LR: 0.001000\nEpoch [56/120] Loss: 0.7623, Val Acc: 0.7000,, LR: 0.000500\nEpoch [57/120] Loss: 0.7014, Val Acc: 0.6980,, LR: 0.000500\nEpoch [58/120] Loss: 0.7335, Val Acc: 0.7070,, LR: 0.000500\nEpoch [59/120] Loss: 0.7350, Val Acc: 0.7020,, LR: 0.000500\nEpoch [60/120] Loss: 0.7197, Val Acc: 0.7040,, LR: 0.000500\nEpoch [61/120] Loss: 0.7195, Val Acc: 0.6760,, LR: 0.000500\nEpoch [62/120] Loss: 0.7021, Val Acc: 0.6970,, LR: 0.000500\nEpoch [63/120] Loss: 0.6846, Val Acc: 0.6970,, LR: 0.000500\nEpoch [64/120] Loss: 0.7030, Val Acc: 0.7030,, LR: 0.000500\nEpoch [65/120] Loss: 0.7175, Val Acc: 0.7170,, LR: 0.000500\nEpoch [66/120] Loss: 0.6901, Val Acc: 0.7110,, LR: 0.000500\nEpoch [67/120] Loss: 0.6759, Val Acc: 0.7070,, LR: 0.000500\nEpoch [68/120] Loss: 0.6789, Val Acc: 0.7020,, LR: 0.000500\nEpoch [69/120] Loss: 0.6575, Val Acc: 0.7180,, LR: 0.000500\nEpoch [70/120] Loss: 0.6844, Val Acc: 0.7040,, LR: 0.000500\nEpoch [71/120] Loss: 0.6720, Val Acc: 0.7120,, LR: 0.000500\nEpoch [72/120] Loss: 0.6803, Val Acc: 0.7020,, LR: 0.000500\nEpoch [73/120] Loss: 0.6601, Val Acc: 0.7110,, LR: 0.000500\nEpoch [74/120] Loss: 0.6390, Val Acc: 0.7110,, LR: 0.000500\nEpoch [75/120] Loss: 0.6531, Val Acc: 0.7230,, LR: 0.000500\nEpoch [76/120] Loss: 0.6479, Val Acc: 0.7160,, LR: 0.000500\nEpoch [77/120] Loss: 0.6296, Val Acc: 0.6960,, LR: 0.000500\nEpoch [78/120] Loss: 0.6603, Val Acc: 0.7180,, LR: 0.000500\nEpoch [79/120] Loss: 0.6733, Val Acc: 0.7130,, LR: 0.000500\nEpoch [80/120] Loss: 0.6292, Val Acc: 0.6920,, LR: 0.000500\nEpoch [81/120] Loss: 0.6143, Val Acc: 0.6940,, LR: 0.000500\nEpoch [82/120] Loss: 0.6416, Val Acc: 0.7150,, LR: 0.000500\nEpoch [83/120] Loss: 0.6527, Val Acc: 0.7080,, LR: 0.000500\nEpoch [84/120] Loss: 0.6549, Val Acc: 0.7000,, LR: 0.000500\nEpoch [85/120] Loss: 0.6635, Val Acc: 0.7190,, LR: 0.000500\nEpoch [86/120] Loss: 0.6457, Val Acc: 0.7110,, LR: 0.000500\nEpoch [87/120] Loss: 0.6050, Val Acc: 0.7200,, LR: 0.000500\nEpoch [88/120] Loss: 0.6091, Val Acc: 0.7110,, LR: 0.000500\nEpoch [89/120] Loss: 0.6015, Val Acc: 0.7050,, LR: 0.000500\nEpoch [90/120] Loss: 0.5964, Val Acc: 0.7110,, LR: 0.000500\nEpoch [91/120] Loss: 0.6063, Val Acc: 0.7090,, LR: 0.000500\nEpoch [92/120] Loss: 0.6192, Val Acc: 0.7270,, LR: 0.000500\nEpoch [93/120] Loss: 0.6004, Val Acc: 0.7020,, LR: 0.000500\nEpoch [94/120] Loss: 0.5983, Val Acc: 0.7110,, LR: 0.000500\nEpoch [95/120] Loss: 0.5663, Val Acc: 0.7200,, LR: 0.000500\nEpoch [96/120] Loss: 0.5998, Val Acc: 0.7300,, LR: 0.000500\nEpoch [97/120] Loss: 0.6077, Val Acc: 0.7080,, LR: 0.000500\nEpoch [98/120] Loss: 0.6043, Val Acc: 0.7130,, LR: 0.000500\nEpoch [99/120] Loss: 0.5804, Val Acc: 0.7200,, LR: 0.000500\nEpoch [100/120] Loss: 0.5759, Val Acc: 0.7010,, LR: 0.000500\nEpoch [101/120] Loss: 0.5996, Val Acc: 0.7090,, LR: 0.000500\nEpoch [102/120] Loss: 0.6125, Val Acc: 0.7120,, LR: 0.000500\nEpoch [103/120] Loss: 0.6349, Val Acc: 0.6940,, LR: 0.000500\nEpoch [104/120] Loss: 0.6296, Val Acc: 0.7030,, LR: 0.000500\nEpoch [105/120] Loss: 0.6172, Val Acc: 0.7120,, LR: 0.000500\nEpoch [106/120] Loss: 0.6263, Val Acc: 0.7010,, LR: 0.000500\nEpoch [107/120] Loss: 0.5941, Val Acc: 0.7170,, LR: 0.000500\nEpoch [108/120] Loss: 0.5891, Val Acc: 0.7050,, LR: 0.000500\nEpoch [109/120] Loss: 0.5562, Val Acc: 0.7140,, LR: 0.000500\nEpoch [110/120] Loss: 0.6014, Val Acc: 0.7130,, LR: 0.000500\nEpoch [111/120] Loss: 0.5691, Val Acc: 0.7230,, LR: 0.000500\nEpoch [112/120] Loss: 0.5606, Val Acc: 0.7110,, LR: 0.000500\nEpoch [113/120] Loss: 0.5756, Val Acc: 0.7170,, LR: 0.000500\nEpoch [114/120] Loss: 0.5839, Val Acc: 0.7270,, LR: 0.000500\nEpoch [115/120] Loss: 0.6012, Val Acc: 0.7220,, LR: 0.000500\nEpoch [116/120] Loss: 0.5944, Val Acc: 0.7130,, LR: 0.000500\nEpoch [117/120] Loss: 0.5699, Val Acc: 0.7160,, LR: 0.000500\nEpoch [118/120] Loss: 0.5729, Val Acc: 0.7150,, LR: 0.000250\nEpoch [119/120] Loss: 0.5468, Val Acc: 0.7150,, LR: 0.000250\nEpoch [120/120] Loss: 0.5432, Val Acc: 0.7110,, LR: 0.000250\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"test(triplet_trainer.model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T04:47:47.921946Z","iopub.execute_input":"2025-05-25T04:47:47.922208Z","iopub.status.idle":"2025-05-25T04:47:50.027449Z","shell.execute_reply.started":"2025-05-25T04:47:47.922187Z","shell.execute_reply":"2025-05-25T04:47:50.026705Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"0.698"},"metadata":{}}],"execution_count":60},{"cell_type":"markdown","source":"## Triplet Loss Model with Test Acc: 69.8%","metadata":{}},{"cell_type":"markdown","source":"# Gamma Values Comparison","metadata":{}},{"cell_type":"code","source":"model_name='edgenext_small'\ngamma = 0.2\nembedding_size=512\nmargin_list = (1.0, 0.0, 0.4)\nnum_classes = len(dataset.classes)\nsample_rate = 1\nlr = 1e-3\nweight_decay = 0.05","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:05:22.605402Z","iopub.execute_input":"2025-05-25T03:05:22.606214Z","iopub.status.idle":"2025-05-25T03:05:22.610208Z","shell.execute_reply.started":"2025-05-25T03:05:22.606188Z","shell.execute_reply":"2025-05-25T03:05:22.609468Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch.nn.functional import normalize, linear\nfrom typing import Callable\nimport math\n\nclass CombinedMarginLoss(torch.nn.Module):\n    def __init__(self, s: float, m1: float, m2: float, m3: float):\n        super().__init__()\n        self.s = s\n        self.m1 = m1\n        self.m2 = m2\n        self.m3 = m3\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        one_hot = torch.zeros_like(logits)\n        one_hot.scatter_(1, labels.view(-1, 1), 1)\n\n        # cosine similarity values (logits) must be clamped for arccos stability\n        cosine = logits.clamp(-1 + 1e-7, 1 - 1e-7)\n        theta = cosine.acos()\n\n        # Apply angular margin (ArcFace or SphereFace)\n        if self.m1 != 1.0 or self.m2 != 0.0:\n            theta = self.m1 * theta + self.m2\n            target_logits = theta.cos()\n        else:\n            target_logits = cosine\n\n        # Apply additive cosine margin (CosFace)\n        if self.m3 > 0.0:\n            target_logits -= self.m3\n\n        # Update logits for the ground-truth classes\n        logits = logits.clone()\n        logits[one_hot.bool()] = target_logits[one_hot.bool()]\n\n        # Apply scale\n        logits *= self.s\n        return logits\n\n\nclass SimplePartialFC(torch.nn.Module):\n    def __init__(\n        self,\n        margin_loss: Callable,\n        embedding_size: int,\n        num_classes: int,\n        sample_rate: float = 1.0,\n        fp16: bool = False,\n    ):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.num_classes = num_classes\n        self.sample_rate = sample_rate\n        self.fp16 = fp16\n\n        self.weight = torch.nn.Parameter(torch.randn(num_classes, embedding_size) * 0.01)\n        self.margin_softmax = margin_loss\n        self.ce_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor):\n        labels = labels.long().view(-1)\n\n        if self.sample_rate < 1.0:\n            with torch.no_grad():\n                positive = torch.unique(labels)\n                num_sample = int(self.sample_rate * self.num_classes)\n                all_indices = torch.randperm(self.num_classes, device=embeddings.device)\n                neg_sample = all_indices[~torch.isin(all_indices, positive)][: max(0, num_sample - len(positive))]\n                sample_indices = torch.cat([positive, neg_sample])\n                sample_indices, _ = sample_indices.sort()\n                weight = self.weight[sample_indices]\n                label_map = {old.item(): new for new, old in enumerate(sample_indices)}\n                labels = torch.tensor([label_map[l.item()] for l in labels], device=labels.device)\n            logits = linear(normalize(embeddings), normalize(weight))\n        else:\n            logits = linear(normalize(embeddings), normalize(self.weight))\n\n        logits = logits.clamp(-1, 1)\n        logits = self.margin_softmax(logits, labels)\n        return self.ce_loss(logits, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:05:27.819412Z","iopub.execute_input":"2025-05-25T03:05:27.820207Z","iopub.status.idle":"2025-05-25T03:05:27.832004Z","shell.execute_reply.started":"2025-05-25T03:05:27.820182Z","shell.execute_reply":"2025-05-25T03:05:27.831216Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class LFWTrainer:\n    def __init__(self, model_name, \n                 embedding_size, train_loader, \n                 val_loader, gamma):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Model\n        # self.model = get_model(model_name, num_features=embedding_size)\n        if gamma == 1:\n            self.model = get_timmfrv2(model_name,featdim=embedding_size)\n        else:\n            self.model = replace_linear_with_lowrank_2(\n                get_timmfrv2(model_name,featdim=embedding_size), rank_ratio=gamma)\n        \n        self.model = self.model.to(self.device)\n\n        self.num_classes = num_classes\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n  \n        # Loss\n        self.criterion = CombinedMarginLoss(64, margin_list[0], margin_list[1], margin_list[2])\n        \n        self.module_partial_fc = SimplePartialFC(self.criterion, embedding_size, \n                                         num_classes, sample_rate, False)\n        self.module_partial_fc.train().cuda()\n\n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            params=[{\"params\": self.model.parameters()}, \n                    {\"params\": self.module_partial_fc.parameters()}],\n            lr=lr, weight_decay=weight_decay)\n         \n        # self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=12, verbose=True)\n\n        # Transform\n        self.transform = transforms.Compose([\n            transforms.Resize((112, 112)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        \n    def train(self, num_epochs=10):\n        for epoch in range(num_epochs):\n            self.model.train()\n            running_loss = 0.0\n            \n            for i, (images, labels) in enumerate(self.train_loader):\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                \n                embeddings = self.model(images)\n                loss: torch.Tensor = self.module_partial_fc(embeddings, labels)\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                \n                running_loss += loss.item()\n\n            # Validate\n            val_acc = self.validate()\n            current_lr = self.optimizer.param_groups[0]['lr']\n            epoch_loss = running_loss / len(self.train_loader)\n            print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f},'\n            f', LR: {current_lr:.6f}')\n\n            self.scheduler.step(val_acc)\n            \n    def validate(self):\n        self.model.eval()\n        all_sims = []\n        all_labels = []\n    \n        with torch.no_grad():\n            for img1, img2, label in self.val_loader:\n                img1, img2 = img1.to(self.device), img2.to(self.device)\n                label = label.to(self.device)\n    \n                emb1 = F.normalize(self.model(img1))\n                emb2 = F.normalize(self.model(img2))\n                sim = F.cosine_similarity(emb1, emb2)\n    \n                all_sims.extend(sim.cpu().numpy())\n                all_labels.extend(label.cpu().numpy())\n    \n        all_sims = np.array(all_sims)\n        all_labels = np.array(all_labels)\n    \n        # Find the best threshold\n        best_acc = 0.0\n        best_thresh = 0.0\n        for thresh in np.arange(0, 1.01, 0.01):\n            preds = (all_sims > thresh).astype(int)\n            acc = (preds == all_labels).mean()\n            if acc > best_acc:\n                best_acc = acc\n                best_thresh = thresh\n        return best_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:28:49.759676Z","iopub.execute_input":"2025-05-25T03:28:49.760495Z","iopub.status.idle":"2025-05-25T03:28:49.772734Z","shell.execute_reply.started":"2025-05-25T03:28:49.760466Z","shell.execute_reply":"2025-05-25T03:28:49.772141Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"trainer=LFWTrainer(model_name = model_name, \n                    embedding_size = embedding_size, \n                    train_loader = train_loader, \n                     val_loader = val_loader,\n                     gamma = gamma\n                  )\n\ntrainer.train(num_epochs=200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:07:15.927881Z","iopub.execute_input":"2025-05-25T03:07:15.928541Z","iopub.status.idle":"2025-05-25T03:25:15.627728Z","shell.execute_reply.started":"2025-05-25T03:07:15.928514Z","shell.execute_reply":"2025-05-25T03:25:15.626574Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/200] Loss: 32.5131, Val Acc: 0.5230,, LR: 0.001000\nEpoch [2/200] Loss: 30.8665, Val Acc: 0.5180,, LR: 0.001000\nEpoch [3/200] Loss: 30.6194, Val Acc: 0.5160,, LR: 0.001000\nEpoch [4/200] Loss: 30.4827, Val Acc: 0.5360,, LR: 0.001000\nEpoch [5/200] Loss: 30.2382, Val Acc: 0.5900,, LR: 0.001000\nEpoch [6/200] Loss: 29.8767, Val Acc: 0.6210,, LR: 0.001000\nEpoch [7/200] Loss: 29.4180, Val Acc: 0.6380,, LR: 0.001000\nEpoch [8/200] Loss: 29.0263, Val Acc: 0.6120,, LR: 0.001000\nEpoch [9/200] Loss: 28.1760, Val Acc: 0.6270,, LR: 0.001000\nEpoch [10/200] Loss: 27.1271, Val Acc: 0.6300,, LR: 0.001000\nEpoch [11/200] Loss: 25.8964, Val Acc: 0.6260,, LR: 0.001000\nEpoch [12/200] Loss: 24.6477, Val Acc: 0.6630,, LR: 0.001000\nEpoch [13/200] Loss: 23.9950, Val Acc: 0.6410,, LR: 0.001000\nEpoch [14/200] Loss: 23.3927, Val Acc: 0.6730,, LR: 0.001000\nEpoch [15/200] Loss: 22.5402, Val Acc: 0.6790,, LR: 0.001000\nEpoch [16/200] Loss: 21.6448, Val Acc: 0.7040,, LR: 0.001000\nEpoch [17/200] Loss: 20.7114, Val Acc: 0.7120,, LR: 0.001000\nEpoch [18/200] Loss: 19.5913, Val Acc: 0.7280,, LR: 0.001000\nEpoch [19/200] Loss: 18.4553, Val Acc: 0.7740,, LR: 0.001000\nEpoch [20/200] Loss: 17.2610, Val Acc: 0.7780,, LR: 0.001000\nEpoch [21/200] Loss: 16.1107, Val Acc: 0.7960,, LR: 0.001000\nEpoch [22/200] Loss: 15.4150, Val Acc: 0.7710,, LR: 0.001000\nEpoch [23/200] Loss: 14.4917, Val Acc: 0.8050,, LR: 0.001000\nEpoch [24/200] Loss: 13.2698, Val Acc: 0.7980,, LR: 0.001000\nEpoch [25/200] Loss: 12.2763, Val Acc: 0.8110,, LR: 0.001000\nEpoch [26/200] Loss: 11.5976, Val Acc: 0.7930,, LR: 0.001000\nEpoch [27/200] Loss: 11.0976, Val Acc: 0.7820,, LR: 0.001000\nEpoch [28/200] Loss: 10.2086, Val Acc: 0.8120,, LR: 0.001000\nEpoch [29/200] Loss: 9.4610, Val Acc: 0.8050,, LR: 0.001000\nEpoch [30/200] Loss: 8.8333, Val Acc: 0.8300,, LR: 0.001000\nEpoch [31/200] Loss: 8.4379, Val Acc: 0.8260,, LR: 0.001000\nEpoch [32/200] Loss: 7.6751, Val Acc: 0.8150,, LR: 0.001000\nEpoch [33/200] Loss: 7.3174, Val Acc: 0.8180,, LR: 0.001000\nEpoch [34/200] Loss: 6.8073, Val Acc: 0.8230,, LR: 0.001000\nEpoch [35/200] Loss: 6.3121, Val Acc: 0.8290,, LR: 0.001000\nEpoch [36/200] Loss: 5.8332, Val Acc: 0.8120,, LR: 0.001000\nEpoch [37/200] Loss: 5.4958, Val Acc: 0.8150,, LR: 0.001000\nEpoch [38/200] Loss: 5.1905, Val Acc: 0.8230,, LR: 0.001000\nEpoch [39/200] Loss: 4.8171, Val Acc: 0.8240,, LR: 0.001000\nEpoch [40/200] Loss: 4.6574, Val Acc: 0.8360,, LR: 0.001000\nEpoch [41/200] Loss: 4.0600, Val Acc: 0.8280,, LR: 0.001000\nEpoch [42/200] Loss: 3.6814, Val Acc: 0.8210,, LR: 0.001000\nEpoch [43/200] Loss: 3.3839, Val Acc: 0.8160,, LR: 0.001000\nEpoch [44/200] Loss: 3.1590, Val Acc: 0.8310,, LR: 0.001000\nEpoch [45/200] Loss: 2.9325, Val Acc: 0.8210,, LR: 0.001000\nEpoch [46/200] Loss: 2.8239, Val Acc: 0.8270,, LR: 0.001000\nEpoch [47/200] Loss: 2.6589, Val Acc: 0.8440,, LR: 0.001000\nEpoch [48/200] Loss: 2.4547, Val Acc: 0.8300,, LR: 0.001000\nEpoch [49/200] Loss: 2.1514, Val Acc: 0.8270,, LR: 0.001000\nEpoch [50/200] Loss: 1.8545, Val Acc: 0.8360,, LR: 0.001000\nEpoch [51/200] Loss: 1.7121, Val Acc: 0.8170,, LR: 0.001000\nEpoch [52/200] Loss: 1.4571, Val Acc: 0.8270,, LR: 0.001000\nEpoch [53/200] Loss: 1.3659, Val Acc: 0.8270,, LR: 0.001000\nEpoch [54/200] Loss: 1.2121, Val Acc: 0.8220,, LR: 0.001000\nEpoch [55/200] Loss: 1.0381, Val Acc: 0.8360,, LR: 0.001000\nEpoch [56/200] Loss: 0.8605, Val Acc: 0.8300,, LR: 0.001000\nEpoch [57/200] Loss: 0.8219, Val Acc: 0.8240,, LR: 0.001000\nEpoch [58/200] Loss: 0.7395, Val Acc: 0.8370,, LR: 0.001000\nEpoch [59/200] Loss: 0.6459, Val Acc: 0.8310,, LR: 0.001000\nEpoch [60/200] Loss: 0.4777, Val Acc: 0.8060,, LR: 0.001000\nEpoch [61/200] Loss: 0.2774, Val Acc: 0.8260,, LR: 0.000500\nEpoch [62/200] Loss: 0.1240, Val Acc: 0.8390,, LR: 0.000500\nEpoch [63/200] Loss: 0.0872, Val Acc: 0.8290,, LR: 0.000500\nEpoch [64/200] Loss: 0.0533, Val Acc: 0.8380,, LR: 0.000500\nEpoch [65/200] Loss: 0.0347, Val Acc: 0.8310,, LR: 0.000500\nEpoch [66/200] Loss: 0.0180, Val Acc: 0.8230,, LR: 0.000500\nEpoch [67/200] Loss: 0.0137, Val Acc: 0.8400,, LR: 0.000500\nEpoch [68/200] Loss: 0.0146, Val Acc: 0.8240,, LR: 0.000500\nEpoch [69/200] Loss: 0.0097, Val Acc: 0.8190,, LR: 0.000500\nEpoch [70/200] Loss: 0.0074, Val Acc: 0.8390,, LR: 0.000500\nEpoch [71/200] Loss: 0.0072, Val Acc: 0.8400,, LR: 0.000500\nEpoch [72/200] Loss: 0.0069, Val Acc: 0.8220,, LR: 0.000500\nEpoch [73/200] Loss: 0.0060, Val Acc: 0.8270,, LR: 0.000500\nEpoch [74/200] Loss: 0.0059, Val Acc: 0.8340,, LR: 0.000250\nEpoch [75/200] Loss: 0.0057, Val Acc: 0.8260,, LR: 0.000250\nEpoch [76/200] Loss: 0.0055, Val Acc: 0.8380,, LR: 0.000250\nEpoch [77/200] Loss: 0.0054, Val Acc: 0.8300,, LR: 0.000250\nEpoch [78/200] Loss: 0.0053, Val Acc: 0.8320,, LR: 0.000250\nEpoch [79/200] Loss: 0.0053, Val Acc: 0.8230,, LR: 0.000250\nEpoch [80/200] Loss: 0.0054, Val Acc: 0.8340,, LR: 0.000250\nEpoch [81/200] Loss: 0.0051, Val Acc: 0.8320,, LR: 0.000250\nEpoch [82/200] Loss: 0.0051, Val Acc: 0.8330,, LR: 0.000250\nEpoch [83/200] Loss: 0.0048, Val Acc: 0.8290,, LR: 0.000250\nEpoch [84/200] Loss: 0.0050, Val Acc: 0.8290,, LR: 0.000250\nEpoch [85/200] Loss: 0.0049, Val Acc: 0.8180,, LR: 0.000250\nEpoch [86/200] Loss: 0.0047, Val Acc: 0.8520,, LR: 0.000250\nEpoch [87/200] Loss: 0.0047, Val Acc: 0.8250,, LR: 0.000250\nEpoch [88/200] Loss: 0.0047, Val Acc: 0.8230,, LR: 0.000250\nEpoch [89/200] Loss: 0.0045, Val Acc: 0.8320,, LR: 0.000250\nEpoch [90/200] Loss: 0.0045, Val Acc: 0.8290,, LR: 0.000250\nEpoch [91/200] Loss: 0.0044, Val Acc: 0.8360,, LR: 0.000250\nEpoch [92/200] Loss: 0.0044, Val Acc: 0.8270,, LR: 0.000250\nEpoch [93/200] Loss: 0.0043, Val Acc: 0.8330,, LR: 0.000250\nEpoch [94/200] Loss: 0.0042, Val Acc: 0.8320,, LR: 0.000250\nEpoch [95/200] Loss: 0.0042, Val Acc: 0.8360,, LR: 0.000250\nEpoch [96/200] Loss: 0.0042, Val Acc: 0.8350,, LR: 0.000250\nEpoch [97/200] Loss: 0.0041, Val Acc: 0.8230,, LR: 0.000250\nEpoch [98/200] Loss: 0.0041, Val Acc: 0.8250,, LR: 0.000250\nEpoch [99/200] Loss: 0.0039, Val Acc: 0.8380,, LR: 0.000250\nEpoch [100/200] Loss: 0.0039, Val Acc: 0.8380,, LR: 0.000125\nEpoch [101/200] Loss: 0.0039, Val Acc: 0.8310,, LR: 0.000125\nEpoch [102/200] Loss: 0.0039, Val Acc: 0.8390,, LR: 0.000125\nEpoch [103/200] Loss: 0.0039, Val Acc: 0.8350,, LR: 0.000125\nEpoch [104/200] Loss: 0.0038, Val Acc: 0.8380,, LR: 0.000125\nEpoch [105/200] Loss: 0.0038, Val Acc: 0.8300,, LR: 0.000125\nEpoch [106/200] Loss: 0.0037, Val Acc: 0.8270,, LR: 0.000125\nEpoch [107/200] Loss: 0.0038, Val Acc: 0.8360,, LR: 0.000125\nEpoch [108/200] Loss: 0.0038, Val Acc: 0.8310,, LR: 0.000125\nEpoch [109/200] Loss: 0.0037, Val Acc: 0.8250,, LR: 0.000125\nEpoch [110/200] Loss: 0.0038, Val Acc: 0.8330,, LR: 0.000125\nEpoch [111/200] Loss: 0.0036, Val Acc: 0.8300,, LR: 0.000125\nEpoch [112/200] Loss: 0.0036, Val Acc: 0.8400,, LR: 0.000125\nEpoch [113/200] Loss: 0.0037, Val Acc: 0.8330,, LR: 0.000063\nEpoch [114/200] Loss: 0.0036, Val Acc: 0.8260,, LR: 0.000063\nEpoch [115/200] Loss: 0.0036, Val Acc: 0.8250,, LR: 0.000063\nEpoch [116/200] Loss: 0.0035, Val Acc: 0.8310,, LR: 0.000063\nEpoch [117/200] Loss: 0.0036, Val Acc: 0.8290,, LR: 0.000063\nEpoch [118/200] Loss: 0.0036, Val Acc: 0.8260,, LR: 0.000063\nEpoch [119/200] Loss: 0.0036, Val Acc: 0.8350,, LR: 0.000063\nEpoch [120/200] Loss: 0.0035, Val Acc: 0.8230,, LR: 0.000063\nEpoch [121/200] Loss: 0.0036, Val Acc: 0.8260,, LR: 0.000063\nEpoch [122/200] Loss: 0.0036, Val Acc: 0.8330,, LR: 0.000063\nEpoch [123/200] Loss: 0.0035, Val Acc: 0.8220,, LR: 0.000063\nEpoch [124/200] Loss: 0.0035, Val Acc: 0.8340,, LR: 0.000063\nEpoch [125/200] Loss: 0.0035, Val Acc: 0.8340,, LR: 0.000063\nEpoch [126/200] Loss: 0.0035, Val Acc: 0.8270,, LR: 0.000031\nEpoch [127/200] Loss: 0.0034, Val Acc: 0.8270,, LR: 0.000031\nEpoch [128/200] Loss: 0.0035, Val Acc: 0.8390,, LR: 0.000031\nEpoch [129/200] Loss: 0.0035, Val Acc: 0.8360,, LR: 0.000031\nEpoch [130/200] Loss: 0.0035, Val Acc: 0.8240,, LR: 0.000031\nEpoch [131/200] Loss: 0.0034, Val Acc: 0.8230,, LR: 0.000031\nEpoch [132/200] Loss: 0.0035, Val Acc: 0.8230,, LR: 0.000031\nEpoch [133/200] Loss: 0.0033, Val Acc: 0.8200,, LR: 0.000031\nEpoch [134/200] Loss: 0.0034, Val Acc: 0.8290,, LR: 0.000031\nEpoch [135/200] Loss: 0.0035, Val Acc: 0.8360,, LR: 0.000031\nEpoch [136/200] Loss: 0.0034, Val Acc: 0.8290,, LR: 0.000031\nEpoch [137/200] Loss: 0.0035, Val Acc: 0.8300,, LR: 0.000031\nEpoch [138/200] Loss: 0.0035, Val Acc: 0.8320,, LR: 0.000031\nEpoch [139/200] Loss: 0.0034, Val Acc: 0.8210,, LR: 0.000016\nEpoch [140/200] Loss: 0.0033, Val Acc: 0.8220,, LR: 0.000016\nEpoch [141/200] Loss: 0.0034, Val Acc: 0.8290,, LR: 0.000016\nEpoch [142/200] Loss: 0.0034, Val Acc: 0.8380,, LR: 0.000016\nEpoch [143/200] Loss: 0.0033, Val Acc: 0.8220,, LR: 0.000016\nEpoch [144/200] Loss: 0.0034, Val Acc: 0.8370,, LR: 0.000016\nEpoch [145/200] Loss: 0.0034, Val Acc: 0.8270,, LR: 0.000016\nEpoch [146/200] Loss: 0.0034, Val Acc: 0.8240,, LR: 0.000016\nEpoch [147/200] Loss: 0.0034, Val Acc: 0.8300,, LR: 0.000016\nEpoch [148/200] Loss: 0.0033, Val Acc: 0.8270,, LR: 0.000016\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1829976564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                   )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2749077966.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    876\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_lerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdevice_beta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"test(trainer.model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:27:14.992005Z","iopub.execute_input":"2025-05-25T03:27:14.992786Z","iopub.status.idle":"2025-05-25T03:27:16.990296Z","shell.execute_reply.started":"2025-05-25T03:27:14.992756Z","shell.execute_reply":"2025-05-25T03:27:16.989574Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.81"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"## Gamma 0.2 model with Test Acc: 81%","metadata":{}},{"cell_type":"code","source":"# CosFace Loss Model above, is also Gamma 0.5 Model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T04:09:00.055722Z","iopub.execute_input":"2025-05-25T04:09:00.056413Z","iopub.status.idle":"2025-05-25T04:09:00.059915Z","shell.execute_reply.started":"2025-05-25T04:09:00.056384Z","shell.execute_reply":"2025-05-25T04:09:00.059239Z"}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"## Gamma 0.5 model with Test Acc: 83.7%","metadata":{}},{"cell_type":"code","source":"trainer_full=LFWTrainer(model_name = model_name, \n                    embedding_size = embedding_size, \n                    train_loader = train_loader, \n                     val_loader = val_loader,\n                     gamma = 1\n                  )\n\ntrainer_full.train(num_epochs=120)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:29:42.125805Z","iopub.execute_input":"2025-05-25T03:29:42.126470Z","iopub.status.idle":"2025-05-25T03:44:40.177074Z","shell.execute_reply.started":"2025-05-25T03:29:42.126441Z","shell.execute_reply":"2025-05-25T03:44:40.176307Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/120] Loss: 33.7074, Val Acc: 0.5330,, LR: 0.001000\nEpoch [2/120] Loss: 30.9313, Val Acc: 0.5070,, LR: 0.001000\nEpoch [3/120] Loss: 30.6163, Val Acc: 0.5120,, LR: 0.001000\nEpoch [4/120] Loss: 30.4814, Val Acc: 0.5260,, LR: 0.001000\nEpoch [5/120] Loss: 30.2050, Val Acc: 0.5630,, LR: 0.001000\nEpoch [6/120] Loss: 29.7476, Val Acc: 0.6090,, LR: 0.001000\nEpoch [7/120] Loss: 29.2451, Val Acc: 0.6510,, LR: 0.001000\nEpoch [8/120] Loss: 28.6550, Val Acc: 0.6290,, LR: 0.001000\nEpoch [9/120] Loss: 27.2374, Val Acc: 0.6330,, LR: 0.001000\nEpoch [10/120] Loss: 25.9500, Val Acc: 0.6140,, LR: 0.001000\nEpoch [11/120] Loss: 24.4452, Val Acc: 0.6330,, LR: 0.001000\nEpoch [12/120] Loss: 23.0406, Val Acc: 0.6490,, LR: 0.001000\nEpoch [13/120] Loss: 21.5031, Val Acc: 0.6440,, LR: 0.001000\nEpoch [14/120] Loss: 20.2032, Val Acc: 0.6620,, LR: 0.001000\nEpoch [15/120] Loss: 19.1258, Val Acc: 0.6910,, LR: 0.001000\nEpoch [16/120] Loss: 17.0902, Val Acc: 0.7610,, LR: 0.001000\nEpoch [17/120] Loss: 15.5891, Val Acc: 0.7570,, LR: 0.001000\nEpoch [18/120] Loss: 13.8003, Val Acc: 0.7770,, LR: 0.001000\nEpoch [19/120] Loss: 12.4689, Val Acc: 0.7930,, LR: 0.001000\nEpoch [20/120] Loss: 10.9463, Val Acc: 0.7850,, LR: 0.001000\nEpoch [21/120] Loss: 9.8665, Val Acc: 0.8000,, LR: 0.001000\nEpoch [22/120] Loss: 9.0212, Val Acc: 0.7750,, LR: 0.001000\nEpoch [23/120] Loss: 7.7561, Val Acc: 0.7910,, LR: 0.001000\nEpoch [24/120] Loss: 6.6612, Val Acc: 0.7920,, LR: 0.001000\nEpoch [25/120] Loss: 5.5053, Val Acc: 0.8130,, LR: 0.001000\nEpoch [26/120] Loss: 4.4494, Val Acc: 0.8230,, LR: 0.001000\nEpoch [27/120] Loss: 3.4914, Val Acc: 0.8120,, LR: 0.001000\nEpoch [28/120] Loss: 2.6241, Val Acc: 0.8220,, LR: 0.001000\nEpoch [29/120] Loss: 1.9338, Val Acc: 0.8130,, LR: 0.001000\nEpoch [30/120] Loss: 1.3841, Val Acc: 0.7920,, LR: 0.001000\nEpoch [31/120] Loss: 1.0701, Val Acc: 0.8200,, LR: 0.001000\nEpoch [32/120] Loss: 0.9521, Val Acc: 0.7850,, LR: 0.001000\nEpoch [33/120] Loss: 1.0737, Val Acc: 0.8100,, LR: 0.001000\nEpoch [34/120] Loss: 0.7061, Val Acc: 0.8200,, LR: 0.001000\nEpoch [35/120] Loss: 0.4570, Val Acc: 0.8180,, LR: 0.001000\nEpoch [36/120] Loss: 0.2584, Val Acc: 0.8270,, LR: 0.001000\nEpoch [37/120] Loss: 0.2025, Val Acc: 0.8130,, LR: 0.001000\nEpoch [38/120] Loss: 0.4347, Val Acc: 0.8240,, LR: 0.001000\nEpoch [39/120] Loss: 0.3407, Val Acc: 0.8220,, LR: 0.001000\nEpoch [40/120] Loss: 0.1882, Val Acc: 0.8250,, LR: 0.001000\nEpoch [41/120] Loss: 0.0852, Val Acc: 0.8170,, LR: 0.001000\nEpoch [42/120] Loss: 0.0485, Val Acc: 0.8340,, LR: 0.001000\nEpoch [43/120] Loss: 0.0158, Val Acc: 0.8390,, LR: 0.001000\nEpoch [44/120] Loss: 0.0062, Val Acc: 0.8310,, LR: 0.001000\nEpoch [45/120] Loss: 0.0024, Val Acc: 0.8190,, LR: 0.001000\nEpoch [46/120] Loss: 0.0024, Val Acc: 0.8170,, LR: 0.001000\nEpoch [47/120] Loss: 0.0026, Val Acc: 0.8280,, LR: 0.001000\nEpoch [48/120] Loss: 0.0010, Val Acc: 0.8290,, LR: 0.001000\nEpoch [49/120] Loss: 0.0007, Val Acc: 0.8290,, LR: 0.001000\nEpoch [50/120] Loss: 0.0006, Val Acc: 0.8290,, LR: 0.001000\nEpoch [51/120] Loss: 0.0005, Val Acc: 0.8320,, LR: 0.001000\nEpoch [52/120] Loss: 0.0005, Val Acc: 0.8280,, LR: 0.001000\nEpoch [53/120] Loss: 0.0005, Val Acc: 0.8260,, LR: 0.001000\nEpoch [54/120] Loss: 0.0004, Val Acc: 0.8260,, LR: 0.001000\nEpoch [55/120] Loss: 0.0004, Val Acc: 0.8220,, LR: 0.001000\nEpoch [56/120] Loss: 0.0004, Val Acc: 0.8310,, LR: 0.001000\nEpoch [57/120] Loss: 0.0004, Val Acc: 0.8270,, LR: 0.000500\nEpoch [58/120] Loss: 0.0004, Val Acc: 0.8260,, LR: 0.000500\nEpoch [59/120] Loss: 0.0004, Val Acc: 0.8310,, LR: 0.000500\nEpoch [60/120] Loss: 0.0004, Val Acc: 0.8210,, LR: 0.000500\nEpoch [61/120] Loss: 0.0004, Val Acc: 0.8280,, LR: 0.000500\nEpoch [62/120] Loss: 0.0004, Val Acc: 0.8240,, LR: 0.000500\nEpoch [63/120] Loss: 0.0004, Val Acc: 0.8240,, LR: 0.000500\nEpoch [64/120] Loss: 0.0004, Val Acc: 0.8290,, LR: 0.000500\nEpoch [65/120] Loss: 0.0004, Val Acc: 0.8330,, LR: 0.000500\nEpoch [66/120] Loss: 0.0004, Val Acc: 0.8220,, LR: 0.000500\nEpoch [67/120] Loss: 0.0003, Val Acc: 0.8220,, LR: 0.000500\nEpoch [68/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000500\nEpoch [69/120] Loss: 0.0004, Val Acc: 0.8320,, LR: 0.000500\nEpoch [70/120] Loss: 0.0003, Val Acc: 0.8230,, LR: 0.000250\nEpoch [71/120] Loss: 0.0003, Val Acc: 0.8250,, LR: 0.000250\nEpoch [72/120] Loss: 0.0003, Val Acc: 0.8270,, LR: 0.000250\nEpoch [73/120] Loss: 0.0003, Val Acc: 0.8300,, LR: 0.000250\nEpoch [74/120] Loss: 0.0003, Val Acc: 0.8220,, LR: 0.000250\nEpoch [75/120] Loss: 0.0003, Val Acc: 0.8290,, LR: 0.000250\nEpoch [76/120] Loss: 0.0003, Val Acc: 0.8310,, LR: 0.000250\nEpoch [77/120] Loss: 0.0003, Val Acc: 0.8210,, LR: 0.000250\nEpoch [78/120] Loss: 0.0003, Val Acc: 0.8200,, LR: 0.000250\nEpoch [79/120] Loss: 0.0003, Val Acc: 0.8260,, LR: 0.000250\nEpoch [80/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000250\nEpoch [81/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000250\nEpoch [82/120] Loss: 0.0003, Val Acc: 0.8180,, LR: 0.000250\nEpoch [83/120] Loss: 0.0003, Val Acc: 0.8210,, LR: 0.000125\nEpoch [84/120] Loss: 0.0003, Val Acc: 0.8220,, LR: 0.000125\nEpoch [85/120] Loss: 0.0003, Val Acc: 0.8290,, LR: 0.000125\nEpoch [86/120] Loss: 0.0003, Val Acc: 0.8200,, LR: 0.000125\nEpoch [87/120] Loss: 0.0003, Val Acc: 0.8320,, LR: 0.000125\nEpoch [88/120] Loss: 0.0003, Val Acc: 0.8200,, LR: 0.000125\nEpoch [89/120] Loss: 0.0003, Val Acc: 0.8310,, LR: 0.000125\nEpoch [90/120] Loss: 0.0003, Val Acc: 0.8320,, LR: 0.000125\nEpoch [91/120] Loss: 0.0003, Val Acc: 0.8320,, LR: 0.000125\nEpoch [92/120] Loss: 0.0003, Val Acc: 0.8210,, LR: 0.000125\nEpoch [93/120] Loss: 0.0003, Val Acc: 0.8230,, LR: 0.000125\nEpoch [94/120] Loss: 0.0003, Val Acc: 0.8270,, LR: 0.000125\nEpoch [95/120] Loss: 0.0003, Val Acc: 0.8270,, LR: 0.000125\nEpoch [96/120] Loss: 0.0003, Val Acc: 0.8240,, LR: 0.000063\nEpoch [97/120] Loss: 0.0003, Val Acc: 0.8390,, LR: 0.000063\nEpoch [98/120] Loss: 0.0003, Val Acc: 0.8300,, LR: 0.000063\nEpoch [99/120] Loss: 0.0003, Val Acc: 0.8290,, LR: 0.000063\nEpoch [100/120] Loss: 0.0003, Val Acc: 0.8270,, LR: 0.000063\nEpoch [101/120] Loss: 0.0003, Val Acc: 0.8230,, LR: 0.000063\nEpoch [102/120] Loss: 0.0003, Val Acc: 0.8290,, LR: 0.000063\nEpoch [103/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000063\nEpoch [104/120] Loss: 0.0003, Val Acc: 0.8190,, LR: 0.000063\nEpoch [105/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000063\nEpoch [106/120] Loss: 0.0003, Val Acc: 0.8250,, LR: 0.000063\nEpoch [107/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000063\nEpoch [108/120] Loss: 0.0003, Val Acc: 0.8270,, LR: 0.000063\nEpoch [109/120] Loss: 0.0003, Val Acc: 0.8230,, LR: 0.000031\nEpoch [110/120] Loss: 0.0003, Val Acc: 0.8290,, LR: 0.000031\nEpoch [111/120] Loss: 0.0003, Val Acc: 0.8300,, LR: 0.000031\nEpoch [112/120] Loss: 0.0003, Val Acc: 0.8260,, LR: 0.000031\nEpoch [113/120] Loss: 0.0003, Val Acc: 0.8260,, LR: 0.000031\nEpoch [114/120] Loss: 0.0003, Val Acc: 0.8260,, LR: 0.000031\nEpoch [115/120] Loss: 0.0003, Val Acc: 0.8330,, LR: 0.000031\nEpoch [116/120] Loss: 0.0003, Val Acc: 0.8280,, LR: 0.000031\nEpoch [117/120] Loss: 0.0003, Val Acc: 0.8260,, LR: 0.000031\nEpoch [118/120] Loss: 0.0003, Val Acc: 0.8310,, LR: 0.000031\nEpoch [119/120] Loss: 0.0003, Val Acc: 0.8320,, LR: 0.000031\nEpoch [120/120] Loss: 0.0003, Val Acc: 0.8220,, LR: 0.000031\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"test(trainer_full.model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:45:31.053950Z","iopub.execute_input":"2025-05-25T03:45:31.054590Z","iopub.status.idle":"2025-05-25T03:45:33.126910Z","shell.execute_reply.started":"2025-05-25T03:45:31.054558Z","shell.execute_reply":"2025-05-25T03:45:33.126195Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0.839"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"## Gamma 1 model with Test Acc: 83.9%","metadata":{}}]}